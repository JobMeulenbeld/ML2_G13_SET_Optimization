{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ce908f7-70fd-48a8-a053-f12ee8156be2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 13:41:37.986623: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-09 13:41:38.023001: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-12-09 13:41:38.023025: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-12-09 13:41:38.023063: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-12-09 13:41:38.033373: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-09 13:41:38.885949: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "\n",
    "# GPU memory growth\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "636b62f7-d72c-4aea-a9e2-181d62b0d4d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SReLU(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.tl = self.add_weight(\"tl\", shape=(), initializer=tf.keras.initializers.Constant(-1.0))\n",
    "        self.tr = self.add_weight(\"tr\", shape=(), initializer=tf.keras.initializers.Constant(1.0))\n",
    "        self.al = self.add_weight(\"al\", shape=(), initializer=tf.keras.initializers.Constant(0.2))\n",
    "        self.ar = self.add_weight(\"ar\", shape=(), initializer=tf.keras.initializers.Constant(0.2))\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        tl = self.tl\n",
    "        tr = self.tr\n",
    "        al = self.al\n",
    "        ar = self.ar\n",
    "\n",
    "        left = tl + al * (x - tl)\n",
    "        right = tr + ar * (x - tr)\n",
    "        mid = x\n",
    "\n",
    "        return tf.where(x <= tl, left, tf.where(x >= tr, right, mid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fae88e-436f-457f-8c1e-594bcb8e1aec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a4e3dd1-232f-4a13-b13a-0189e16aaf63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MaskedDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, eps=20, use_bias=True, **kwargs):\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = int(units)\n",
    "        self.eps = float(eps)\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n_in = int(input_shape[-1])\n",
    "        n_out = self.units\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\",\n",
    "            shape=(n_in, n_out),\n",
    "            initializer=\"he_uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(\n",
    "                name=\"bias\",\n",
    "                shape=(n_out,),\n",
    "                initializer=\"zeros\",\n",
    "                trainable=True,\n",
    "            )\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        # mask\n",
    "        p = self.eps * (n_in + n_out) / (n_in * n_out)\n",
    "        p_eff = min(p, 1.0)\n",
    "\n",
    "        M = (np.random.rand(n_in, n_out) < p_eff).astype(np.float32)\n",
    "\n",
    "        for j in range(n_out):\n",
    "            if M[:, j].sum() == 0:\n",
    "                i = np.random.randint(0, n_in)\n",
    "                M[i, j] = 1.0\n",
    "\n",
    "        self.mask = tf.Variable(\n",
    "            initial_value=M,\n",
    "            trainable=False,\n",
    "            dtype=tf.float32,\n",
    "            name=\"mask\",\n",
    "        )\n",
    "\n",
    "        # initial zeros\n",
    "        self.apply_mask()\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        w_eff = self.kernel * self.mask\n",
    "        out = tf.linalg.matmul(inputs, w_eff)\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "        return out\n",
    "\n",
    "    def apply_mask(self):\n",
    "        self.kernel.assign(self.kernel * self.mask)\n",
    "\n",
    "    def prune_and_regrow(self, zeta=0.3, strategy=\"random\", alpha=0.5):\n",
    "\n",
    "        k = self.kernel.numpy().copy()\n",
    "        m = self.mask.numpy().copy()\n",
    "\n",
    "        nz = np.argwhere(m > 0)\n",
    "        num_nz = nz.shape[0]\n",
    "        if num_nz == 0:\n",
    "            return\n",
    "\n",
    "        n_prune = int(zeta * num_nz)\n",
    "        if n_prune <= 0:\n",
    "            return\n",
    "\n",
    "        #PRUNE SMALLEST ABS(W) \n",
    "        w_nz = np.abs(k[m > 0])\n",
    "        thresh = np.partition(w_nz, n_prune - 1)[n_prune - 1]\n",
    "\n",
    "        prune_candidates = np.argwhere((m > 0) & (np.abs(k) <= thresh))\n",
    "        if prune_candidates.shape[0] > n_prune:\n",
    "            sel = np.random.choice(prune_candidates.shape[0], size=n_prune, replace=False)\n",
    "            prune_idx = prune_candidates[sel]\n",
    "        else:\n",
    "            prune_idx = prune_candidates\n",
    "\n",
    "        for i, j in prune_idx:\n",
    "            m[i, j] = 0.0\n",
    "            k[i, j] = 0.0\n",
    "\n",
    "        # REGROW \n",
    "        zeros = np.argwhere(m == 0)\n",
    "        if zeros.shape[0] == 0:\n",
    "            self.mask.assign(m.astype(np.float32))\n",
    "            self.kernel.assign(k.astype(np.float32))\n",
    "            return\n",
    "\n",
    "        n_prune = min(n_prune, zeros.shape[0])\n",
    "\n",
    "        #TRYING MAGNITUDE BASED\n",
    "        if strategy == \"importance\":\n",
    "            \n",
    "            w_eff = k * m\n",
    "            I_in = np.sum(np.abs(w_eff), axis=1)  \n",
    "            I_out = np.sum(np.abs(w_eff), axis=0) \n",
    "\n",
    "            base = I_in[zeros[:, 0]] * I_out[zeros[:, 1]]\n",
    "            mean_base = base.mean() if base.size else 0.0\n",
    "            scores = alpha * base + (1.0 - alpha) * mean_base\n",
    "\n",
    "            # Robust sampling to avoid alpha=1.0 failure\n",
    "            pos = scores > 0\n",
    "            pos_idx = np.where(pos)[0]\n",
    "\n",
    "            if len(pos_idx) >= n_prune:\n",
    "                pos_scores = scores[pos_idx]\n",
    "                ssum = pos_scores.sum()\n",
    "                if ssum <= 0:\n",
    "                    grow_pick = np.random.choice(len(zeros), n_prune, replace=False)\n",
    "                else:\n",
    "                    pos_probs = pos_scores / ssum\n",
    "                    grow_pick = np.random.choice(pos_idx, n_prune, replace=False, p=pos_probs)\n",
    "            else:\n",
    "                \n",
    "                chosen = list(pos_idx)\n",
    "\n",
    "               \n",
    "                remaining = n_prune - len(pos_idx)\n",
    "                rest_idx = np.where(~pos)[0]\n",
    "                if remaining > 0 and len(rest_idx) > 0:\n",
    "                    extra = np.random.choice(rest_idx, remaining, replace=False)\n",
    "                    chosen = np.concatenate([np.array(chosen, dtype=int), extra]) if len(chosen) else extra\n",
    "                grow_pick = np.array(chosen, dtype=int)\n",
    "\n",
    "        else:\n",
    "            # RANDOM REGROWTH\n",
    "            grow_pick = np.random.choice(len(zeros), n_prune, replace=False)\n",
    "\n",
    "        grow_idx = zeros[grow_pick]\n",
    "\n",
    "        # He-uniform init for new edges\n",
    "        limit = np.sqrt(6.0 / self.n_in)\n",
    "        for i, j in grow_idx:\n",
    "            m[i, j] = 1.0\n",
    "            k[i, j] = np.random.uniform(-limit, limit)\n",
    "\n",
    "        self.mask.assign(m.astype(np.float32))\n",
    "        self.kernel.assign(k.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bacce43e-8b37-4688-a59b-b0ebe52aad1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SETModel(tf.keras.Model):\n",
    "    def __init__(self, *args, weight_decay=0.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.weight_decay = float(weight_decay)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        y = tf.cast(y, tf.int32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred)\n",
    "\n",
    "            # L2 on ACTIVE EDGES ONLY (WEIGHT DECAY 0.002 IN PAPER!!)\n",
    "            if self.weight_decay > 0:\n",
    "                wd = 0.0\n",
    "                for layer in self.layers:\n",
    "                    if isinstance(layer, MaskedDense):\n",
    "                        wd += tf.reduce_sum(tf.square(layer.kernel * layer.mask))\n",
    "                loss += self.weight_decay * wd\n",
    "\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        grads = list(grads)\n",
    "\n",
    "        # MASK GRADIENT FOR KERNELS\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, MaskedDense):\n",
    "                k = layer.kernel\n",
    "                m = layer.mask\n",
    "                for i, var in enumerate(self.trainable_variables):\n",
    "                    if var is k and grads[i] is not None:\n",
    "                        grads[i] = grads[i] * m\n",
    "                        break\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        # HARD ENFORCE ZERO (FOR MOMENTUM)\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, MaskedDense):\n",
    "                layer.apply_mask()\n",
    "\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        out = {m.name: m.result() for m in self.metrics}\n",
    "        out[\"loss\"] = loss\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5f644b9-44ac-4543-9207-06f54f6cf7c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SETCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, zeta=0.3, strategy=\"random\", alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.zeta = float(zeta)\n",
    "        self.strategy = strategy\n",
    "        self.alpha = float(alpha)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, MaskedDense):\n",
    "                layer.prune_and_regrow(\n",
    "                    zeta=self.zeta,\n",
    "                    strategy=self.strategy,\n",
    "                    alpha=self.alpha\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1d92d18-a2ff-4183-8987-dedeb2c639fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IN PAPER: 784-1000-1000-1000-10\n",
    "def build_set_mlp(eps=20, weight_decay=2e-4, dropout=0.3):\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(784,))\n",
    "\n",
    "    x = MaskedDense(1000, eps=eps, name=\"md1\")(inputs)\n",
    "    x = SReLU(name=\"srelu1\")(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "    x = MaskedDense(1000, eps=eps, name=\"md2\")(x)\n",
    "    x = SReLU(name=\"srelu2\")(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "    x = MaskedDense(1000, eps=eps, name=\"md3\")(x)\n",
    "    x = SReLU(name=\"srelu3\")(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "    outputs = MaskedDense(10, eps=eps, name=\"md4\")(x)\n",
    "\n",
    "    return SETModel(inputs, outputs, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79e164ff-aff4-4115-9381-7d3824985ded",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_dense_mlp(weight_decay=2e-4, dropout=0.3):\n",
    "    reg = tf.keras.regularizers.l2(weight_decay)\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(784,))\n",
    "\n",
    "    x = tf.keras.layers.Dense(1000, kernel_regularizer=reg)(inputs)\n",
    "    x = SReLU(name=\"srelu1_dense\")(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(1000, kernel_regularizer=reg)(x)\n",
    "    x = SReLU(name=\"srelu2_dense\")(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(1000, kernel_regularizer=reg)(x)\n",
    "    x = SReLU(name=\"srelu3_dense\")(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(10, kernel_regularizer=reg)(x)\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8781e6ad-9034-4567-a82b-eeec74e8c6d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_active_weights(model):\n",
    "\n",
    "    total = 0\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, MaskedDense):\n",
    "            total += int(tf.reduce_sum(layer.mask).numpy())\n",
    "            if layer.bias is not None:\n",
    "                total += int(layer.bias.shape[0])\n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            total += int(np.prod(layer.kernel.shape))\n",
    "            if layer.bias is not None:\n",
    "                total += int(layer.bias.shape[0])\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5917631-e6c7-413f-803d-0d1db398c866",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestEvalCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, x_test, y_test, every=5, batch_size=256):\n",
    "        super().__init__()\n",
    "        self.every = int(every)\n",
    "        self.test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.every <= 0:\n",
    "            return\n",
    "        if (epoch + 1) % self.every != 0:\n",
    "            return\n",
    "        test_loss, test_acc = self.model.evaluate(self.test_ds, verbose=0)\n",
    "        print(f\"  >> Test@{epoch+1:02d} - loss: {test_loss:.4f} - acc: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40a3a614-8489-42fd-a99c-d2289410d178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_and_report(\n",
    "    name,\n",
    "    model,\n",
    "    x_train, y_train, x_test, y_test,\n",
    "    callbacks=None,\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    test_every=5,\n",
    "    test_batch_size=256,\n",
    "    target_test_acc=None,   \n",
    "    min_epoch=5 ):\n",
    "    \n",
    "    callbacks = callbacks or []\n",
    "\n",
    "\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(test_batch_size)\n",
    "\n",
    "    class TestEvalCallback(tf.keras.callbacks.Callback):\n",
    "        def __init__(self, test_ds, every=1):\n",
    "            super().__init__()\n",
    "            self.test_ds = test_ds\n",
    "            self.every = int(every)\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            e = epoch + 1\n",
    "            if self.every > 1 and (e % self.every != 0):\n",
    "                return\n",
    "            test_loss, test_acc = self.model.evaluate(self.test_ds, verbose=0)\n",
    "            print(f\"  >> Test@{e:03d} - loss: {test_loss:.4f} - acc: {test_acc:.4f}\")\n",
    "\n",
    "    class StopOnTestAccuracy(tf.keras.callbacks.Callback):\n",
    "        def __init__(self, test_ds, target, min_epoch=1, every=1):\n",
    "            super().__init__()\n",
    "            self.test_ds = test_ds\n",
    "            self.target = target\n",
    "            self.min_epoch = int(min_epoch)\n",
    "            self.every = int(every)\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            if self.target is None:\n",
    "                return\n",
    "            e = epoch + 1\n",
    "            if e < self.min_epoch:\n",
    "                return\n",
    "            if self.every > 1 and (e % self.every != 0):\n",
    "                return\n",
    "\n",
    "            _, test_acc = self.model.evaluate(self.test_ds, verbose=0)\n",
    "            if test_acc >= self.target:\n",
    "                print(f\"  >> Reached target test acc {self.target:.4f} at epoch {e}. Stopping.\")\n",
    "                self.model.stop_training = True\n",
    "\n",
    "    # Add callbacks\n",
    "    callbacks = callbacks + [\n",
    "        TestEvalCallback(test_ds, every=test_every),\n",
    "        StopOnTestAccuracy(test_ds, target_test_acc, min_epoch=min_epoch, every=1),\n",
    "    ]\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    print(f\"\\n\\n Training: {name}\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    hist = model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=callbacks,\n",
    "        verbose=2\n",
    "    )\n",
    "    train_time = time.time() - t0\n",
    "\n",
    "    final_test_loss, final_test_acc = model.evaluate(test_ds, verbose=0)\n",
    "\n",
    "    nW = count_active_weights(model)\n",
    "    best_val = max(hist.history.get(\"val_accuracy\", [float(\"nan\")]))\n",
    "\n",
    "    print(f\"Done: {name} | Final TestAcc={final_test_acc:.4f} | nW={nW}\")\n",
    "\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"nW\": nW,\n",
    "        \"Test Acc\": float(final_test_acc),\n",
    "        \"Best Val Acc\": float(best_val),\n",
    "        \"Train Time (s)\": float(train_time),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25bc1693-40d1-43f9-ba4a-9d0b1632f727",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_all_models(\n",
    "    eps=20,\n",
    "    zeta=0.3,\n",
    "    alpha=0.5,\n",
    "    epochs=100,\n",
    "    seed=42,\n",
    "    weight_decay=2e-4,\n",
    "    dropout=0.3\n",
    "):\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # MNIST preprocessing\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255.0\n",
    "    x_test  = x_test.reshape(-1, 784).astype(\"float32\") / 255.0\n",
    "    y_train = y_train.astype(\"int32\")\n",
    "    y_test  = y_test.astype(\"int32\")\n",
    "\n",
    "    results = []\n",
    "    # 1) Dense\n",
    "    tf.keras.backend.clear_session()\n",
    "    dense = build_dense_mlp(weight_decay=weight_decay, dropout=dropout)\n",
    "    results.append(run_and_report(\n",
    "        \"Dense MLP\",\n",
    "        dense, x_train, y_train, x_test, y_test,\n",
    "        callbacks=[],\n",
    "        epochs=epochs,\n",
    "        # optional early stop on test acc\n",
    "        target_test_acc=0.985,  \n",
    "        min_epoch=10\n",
    "    ))\n",
    "    del dense; gc.collect()\n",
    "\n",
    "    # 2) FixProb, no SET!!!\n",
    "    tf.keras.backend.clear_session()\n",
    "    fixprob = build_set_mlp(eps=eps, weight_decay=weight_decay, dropout=dropout)\n",
    "    results.append(run_and_report(\n",
    "        \"FixProb\",\n",
    "        fixprob, x_train, y_train, x_test, y_test,\n",
    "        callbacks=[],\n",
    "        epochs=epochs,\n",
    "        target_test_acc=0.985,  \n",
    "        min_epoch=10\n",
    "    ))\n",
    "    del fixprob; gc.collect()\n",
    "\n",
    "    # 3) SET RANDOM\n",
    "    tf.keras.backend.clear_session()\n",
    "    set_random = build_set_mlp(eps=eps, weight_decay=weight_decay, dropout=dropout)\n",
    "    cb_random = SETCallback(zeta=zeta, strategy=\"random\", alpha=0.0)\n",
    "    results.append(run_and_report(\n",
    "        \"SET (RAND)\",\n",
    "        set_random, x_train, y_train, x_test, y_test,\n",
    "        callbacks=[cb_random],\n",
    "        epochs=epochs,\n",
    "        target_test_acc=0.985,   \n",
    "        min_epoch=10\n",
    "    ))\n",
    "    del set_random; gc.collect()\n",
    "\n",
    "    # 4) SET IMP MAGNITUDE\n",
    "    tf.keras.backend.clear_session()\n",
    "    set_imp = build_set_mlp(eps=eps, weight_decay=weight_decay, dropout=dropout)\n",
    "    cb_imp = SETCallback(zeta=zeta, strategy=\"importance\", alpha=alpha)\n",
    "    results.append(run_and_report(\n",
    "        f\"SET (IMP, Î±={alpha})\",\n",
    "        set_imp, x_train, y_train, x_test, y_test,\n",
    "        callbacks=[cb_imp],\n",
    "        epochs=epochs,\n",
    "        target_test_acc=0.985,   \n",
    "        min_epoch=10\n",
    "    ))\n",
    "    del set_imp; gc.collect()\n",
    "\n",
    "\n",
    "    print(\"\\n Final Table\")\n",
    "    for r in results:\n",
    "        print(\n",
    "            f\"{r['Model']:<32} | \"\n",
    "            f\"nW={r['nW']:<10} | \"\n",
    "            f\"TestAcc={r['Test Acc']:.4f} | \"\n",
    "            f\"BestVal={r['Best Val Acc']:.4f} | \"\n",
    "            f\"Time={r['Train Time (s)']:.1f}s\"\n",
    "        )\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffc00ca4-b713-4b3e-8898-68a46eebdf70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Training: Dense MLP\n",
      "Epoch 1/200\n",
      "422/422 - 8s - loss: 1.0117 - accuracy: 0.8671 - val_loss: 0.7980 - val_accuracy: 0.9337 - 8s/epoch - 18ms/step\n",
      "Epoch 2/200\n",
      "422/422 - 6s - loss: 0.8443 - accuracy: 0.9136 - val_loss: 0.7124 - val_accuracy: 0.9525 - 6s/epoch - 13ms/step\n",
      "Epoch 3/200\n",
      "422/422 - 6s - loss: 0.7389 - accuracy: 0.9405 - val_loss: 0.6514 - val_accuracy: 0.9668 - 6s/epoch - 13ms/step\n",
      "Epoch 4/200\n",
      "422/422 - 5s - loss: 0.6757 - accuracy: 0.9544 - val_loss: 0.6138 - val_accuracy: 0.9733 - 5s/epoch - 13ms/step\n",
      "Epoch 5/200\n",
      "  >> Test@005 - loss: 0.5963 - acc: 0.9698\n",
      "422/422 - 6s - loss: 0.6307 - accuracy: 0.9638 - val_loss: 0.5870 - val_accuracy: 0.9755 - 6s/epoch - 14ms/step\n",
      "Epoch 6/200\n",
      "422/422 - 6s - loss: 0.5980 - accuracy: 0.9692 - val_loss: 0.5687 - val_accuracy: 0.9760 - 6s/epoch - 13ms/step\n",
      "Epoch 7/200\n",
      "422/422 - 6s - loss: 0.5685 - accuracy: 0.9734 - val_loss: 0.5527 - val_accuracy: 0.9768 - 6s/epoch - 13ms/step\n",
      "Epoch 8/200\n",
      "422/422 - 6s - loss: 0.5470 - accuracy: 0.9761 - val_loss: 0.5308 - val_accuracy: 0.9812 - 6s/epoch - 13ms/step\n",
      "Epoch 9/200\n",
      "422/422 - 6s - loss: 0.5273 - accuracy: 0.9781 - val_loss: 0.5156 - val_accuracy: 0.9820 - 6s/epoch - 13ms/step\n",
      "Epoch 10/200\n",
      "  >> Test@010 - loss: 0.5013 - acc: 0.9791\n",
      "422/422 - 6s - loss: 0.5065 - accuracy: 0.9799 - val_loss: 0.4984 - val_accuracy: 0.9823 - 6s/epoch - 15ms/step\n",
      "Epoch 11/200\n",
      "422/422 - 6s - loss: 0.4880 - accuracy: 0.9817 - val_loss: 0.4851 - val_accuracy: 0.9812 - 6s/epoch - 14ms/step\n",
      "Epoch 12/200\n",
      "422/422 - 6s - loss: 0.4721 - accuracy: 0.9831 - val_loss: 0.4693 - val_accuracy: 0.9820 - 6s/epoch - 14ms/step\n",
      "Epoch 13/200\n",
      "422/422 - 6s - loss: 0.4543 - accuracy: 0.9851 - val_loss: 0.4583 - val_accuracy: 0.9837 - 6s/epoch - 14ms/step\n",
      "Epoch 14/200\n",
      "422/422 - 6s - loss: 0.4395 - accuracy: 0.9863 - val_loss: 0.4446 - val_accuracy: 0.9832 - 6s/epoch - 14ms/step\n",
      "Epoch 15/200\n",
      "  >> Test@015 - loss: 0.4349 - acc: 0.9816\n",
      "422/422 - 6s - loss: 0.4242 - accuracy: 0.9879 - val_loss: 0.4353 - val_accuracy: 0.9845 - 6s/epoch - 15ms/step\n",
      "Epoch 16/200\n",
      "422/422 - 6s - loss: 0.4117 - accuracy: 0.9879 - val_loss: 0.4233 - val_accuracy: 0.9840 - 6s/epoch - 14ms/step\n",
      "Epoch 17/200\n",
      "422/422 - 6s - loss: 0.3986 - accuracy: 0.9892 - val_loss: 0.4095 - val_accuracy: 0.9858 - 6s/epoch - 14ms/step\n",
      "Epoch 18/200\n",
      "422/422 - 6s - loss: 0.3854 - accuracy: 0.9898 - val_loss: 0.4023 - val_accuracy: 0.9850 - 6s/epoch - 14ms/step\n",
      "Epoch 19/200\n",
      "422/422 - 6s - loss: 0.3738 - accuracy: 0.9907 - val_loss: 0.3900 - val_accuracy: 0.9858 - 6s/epoch - 14ms/step\n",
      "Epoch 20/200\n",
      "  >> Test@020 - loss: 0.3829 - acc: 0.9837\n",
      "422/422 - 6s - loss: 0.3633 - accuracy: 0.9908 - val_loss: 0.3778 - val_accuracy: 0.9865 - 6s/epoch - 15ms/step\n",
      "Epoch 21/200\n",
      "422/422 - 6s - loss: 0.3536 - accuracy: 0.9909 - val_loss: 0.3717 - val_accuracy: 0.9860 - 6s/epoch - 14ms/step\n",
      "Epoch 22/200\n",
      "422/422 - 6s - loss: 0.3424 - accuracy: 0.9916 - val_loss: 0.3590 - val_accuracy: 0.9873 - 6s/epoch - 14ms/step\n",
      "Epoch 23/200\n",
      "422/422 - 6s - loss: 0.3325 - accuracy: 0.9928 - val_loss: 0.3531 - val_accuracy: 0.9873 - 6s/epoch - 14ms/step\n",
      "Epoch 24/200\n",
      "422/422 - 6s - loss: 0.3231 - accuracy: 0.9931 - val_loss: 0.3434 - val_accuracy: 0.9863 - 6s/epoch - 14ms/step\n",
      "Epoch 25/200\n",
      "  >> Test@025 - loss: 0.3384 - acc: 0.9846\n",
      "422/422 - 6s - loss: 0.3124 - accuracy: 0.9937 - val_loss: 0.3343 - val_accuracy: 0.9860 - 6s/epoch - 15ms/step\n",
      "Epoch 26/200\n",
      "422/422 - 6s - loss: 0.3048 - accuracy: 0.9936 - val_loss: 0.3267 - val_accuracy: 0.9863 - 6s/epoch - 14ms/step\n",
      "Epoch 27/200\n",
      "422/422 - 6s - loss: 0.2950 - accuracy: 0.9944 - val_loss: 0.3186 - val_accuracy: 0.9867 - 6s/epoch - 14ms/step\n",
      "Epoch 28/200\n",
      "422/422 - 6s - loss: 0.2873 - accuracy: 0.9946 - val_loss: 0.3096 - val_accuracy: 0.9867 - 6s/epoch - 14ms/step\n",
      "Epoch 29/200\n",
      "422/422 - 6s - loss: 0.2800 - accuracy: 0.9940 - val_loss: 0.3058 - val_accuracy: 0.9860 - 6s/epoch - 14ms/step\n",
      "Epoch 30/200\n",
      "  >> Test@030 - loss: 0.3031 - acc: 0.9840\n",
      "422/422 - 7s - loss: 0.2713 - accuracy: 0.9951 - val_loss: 0.2979 - val_accuracy: 0.9863 - 7s/epoch - 16ms/step\n",
      "Epoch 31/200\n",
      "422/422 - 6s - loss: 0.2647 - accuracy: 0.9952 - val_loss: 0.2912 - val_accuracy: 0.9872 - 6s/epoch - 14ms/step\n",
      "Epoch 32/200\n",
      "422/422 - 6s - loss: 0.2568 - accuracy: 0.9954 - val_loss: 0.2840 - val_accuracy: 0.9867 - 6s/epoch - 14ms/step\n",
      "Epoch 33/200\n",
      "422/422 - 6s - loss: 0.2491 - accuracy: 0.9961 - val_loss: 0.2744 - val_accuracy: 0.9870 - 6s/epoch - 14ms/step\n",
      "Epoch 34/200\n",
      "422/422 - 6s - loss: 0.2420 - accuracy: 0.9961 - val_loss: 0.2686 - val_accuracy: 0.9875 - 6s/epoch - 14ms/step\n",
      "Epoch 35/200\n",
      "  >> Test@035 - loss: 0.2670 - acc: 0.9851\n",
      "  >> Reached target test acc 0.9850 at epoch 35. Stopping.\n",
      "422/422 - 6s - loss: 0.2363 - accuracy: 0.9962 - val_loss: 0.2642 - val_accuracy: 0.9863 - 6s/epoch - 15ms/step\n",
      "Done: Dense MLP | Final TestAcc=0.9851 | nW=2797010\n",
      "\n",
      "\n",
      " Training: FixProb\n",
      "Epoch 1/200\n",
      "422/422 - 9s - loss: 2.2037 - accuracy: 0.3300 - val_loss: 1.9741 - val_accuracy: 0.6402 - 9s/epoch - 21ms/step\n",
      "Epoch 2/200\n",
      "422/422 - 6s - loss: 1.2793 - accuracy: 0.6829 - val_loss: 0.6514 - val_accuracy: 0.8370 - 6s/epoch - 15ms/step\n",
      "Epoch 3/200\n",
      "422/422 - 6s - loss: 0.6172 - accuracy: 0.8192 - val_loss: 0.4169 - val_accuracy: 0.8910 - 6s/epoch - 14ms/step\n",
      "Epoch 4/200\n",
      "422/422 - 6s - loss: 0.4901 - accuracy: 0.8562 - val_loss: 0.3516 - val_accuracy: 0.9063 - 6s/epoch - 15ms/step\n",
      "Epoch 5/200\n",
      "  >> Test@005 - loss: 0.3650 - acc: 0.8957\n",
      "422/422 - 7s - loss: 0.4388 - accuracy: 0.8737 - val_loss: 0.3214 - val_accuracy: 0.9133 - 7s/epoch - 16ms/step\n",
      "Epoch 6/200\n",
      "422/422 - 6s - loss: 0.4117 - accuracy: 0.8807 - val_loss: 0.3054 - val_accuracy: 0.9160 - 6s/epoch - 14ms/step\n",
      "Epoch 7/200\n",
      "422/422 - 6s - loss: 0.3934 - accuracy: 0.8853 - val_loss: 0.2895 - val_accuracy: 0.9187 - 6s/epoch - 15ms/step\n",
      "Epoch 8/200\n",
      "422/422 - 6s - loss: 0.3797 - accuracy: 0.8907 - val_loss: 0.2817 - val_accuracy: 0.9207 - 6s/epoch - 15ms/step\n",
      "Epoch 9/200\n",
      "422/422 - 6s - loss: 0.3696 - accuracy: 0.8935 - val_loss: 0.2766 - val_accuracy: 0.9233 - 6s/epoch - 15ms/step\n",
      "Epoch 10/200\n",
      "  >> Test@010 - loss: 0.3086 - acc: 0.9108\n",
      "422/422 - 7s - loss: 0.3632 - accuracy: 0.8947 - val_loss: 0.2702 - val_accuracy: 0.9242 - 7s/epoch - 16ms/step\n",
      "Epoch 11/200\n",
      "422/422 - 7s - loss: 0.3554 - accuracy: 0.8964 - val_loss: 0.2656 - val_accuracy: 0.9253 - 7s/epoch - 16ms/step\n",
      "Epoch 12/200\n",
      "422/422 - 7s - loss: 0.3488 - accuracy: 0.8987 - val_loss: 0.2602 - val_accuracy: 0.9272 - 7s/epoch - 16ms/step\n",
      "Epoch 13/200\n",
      "422/422 - 6s - loss: 0.3419 - accuracy: 0.9013 - val_loss: 0.2587 - val_accuracy: 0.9273 - 6s/epoch - 15ms/step\n",
      "Epoch 14/200\n",
      "422/422 - 7s - loss: 0.3386 - accuracy: 0.9016 - val_loss: 0.2528 - val_accuracy: 0.9283 - 7s/epoch - 16ms/step\n",
      "Epoch 15/200\n",
      "  >> Test@015 - loss: 0.2865 - acc: 0.9178\n",
      "422/422 - 7s - loss: 0.3329 - accuracy: 0.9040 - val_loss: 0.2498 - val_accuracy: 0.9290 - 7s/epoch - 17ms/step\n",
      "Epoch 16/200\n",
      "422/422 - 6s - loss: 0.3247 - accuracy: 0.9056 - val_loss: 0.2432 - val_accuracy: 0.9300 - 6s/epoch - 15ms/step\n",
      "Epoch 17/200\n",
      "422/422 - 6s - loss: 0.3201 - accuracy: 0.9063 - val_loss: 0.2373 - val_accuracy: 0.9325 - 6s/epoch - 15ms/step\n",
      "Epoch 18/200\n",
      "422/422 - 7s - loss: 0.3115 - accuracy: 0.9085 - val_loss: 0.2318 - val_accuracy: 0.9335 - 7s/epoch - 16ms/step\n",
      "Epoch 19/200\n",
      "422/422 - 7s - loss: 0.3056 - accuracy: 0.9105 - val_loss: 0.2221 - val_accuracy: 0.9378 - 7s/epoch - 16ms/step\n",
      "Epoch 20/200\n",
      "  >> Test@020 - loss: 0.2496 - acc: 0.9268\n",
      "422/422 - 7s - loss: 0.2970 - accuracy: 0.9117 - val_loss: 0.2147 - val_accuracy: 0.9392 - 7s/epoch - 16ms/step\n",
      "Epoch 21/200\n",
      "422/422 - 7s - loss: 0.2873 - accuracy: 0.9159 - val_loss: 0.2070 - val_accuracy: 0.9413 - 7s/epoch - 16ms/step\n",
      "Epoch 22/200\n",
      "422/422 - 7s - loss: 0.2763 - accuracy: 0.9192 - val_loss: 0.1967 - val_accuracy: 0.9450 - 7s/epoch - 16ms/step\n",
      "Epoch 23/200\n",
      "422/422 - 6s - loss: 0.2692 - accuracy: 0.9211 - val_loss: 0.1874 - val_accuracy: 0.9497 - 6s/epoch - 15ms/step\n",
      "Epoch 24/200\n",
      "422/422 - 7s - loss: 0.2560 - accuracy: 0.9247 - val_loss: 0.1814 - val_accuracy: 0.9503 - 7s/epoch - 16ms/step\n",
      "Epoch 25/200\n",
      "  >> Test@025 - loss: 0.1986 - acc: 0.9410\n",
      "422/422 - 7s - loss: 0.2467 - accuracy: 0.9280 - val_loss: 0.1689 - val_accuracy: 0.9525 - 7s/epoch - 17ms/step\n",
      "Epoch 26/200\n",
      "422/422 - 6s - loss: 0.2396 - accuracy: 0.9280 - val_loss: 0.1644 - val_accuracy: 0.9540 - 6s/epoch - 15ms/step\n",
      "Epoch 27/200\n",
      "422/422 - 7s - loss: 0.2310 - accuracy: 0.9318 - val_loss: 0.1535 - val_accuracy: 0.9568 - 7s/epoch - 16ms/step\n",
      "Epoch 28/200\n",
      "422/422 - 7s - loss: 0.2206 - accuracy: 0.9348 - val_loss: 0.1453 - val_accuracy: 0.9585 - 7s/epoch - 16ms/step\n",
      "Epoch 29/200\n",
      "422/422 - 7s - loss: 0.2156 - accuracy: 0.9360 - val_loss: 0.1429 - val_accuracy: 0.9595 - 7s/epoch - 16ms/step\n",
      "Epoch 30/200\n",
      "  >> Test@030 - loss: 0.1614 - acc: 0.9524\n",
      "422/422 - 7s - loss: 0.2088 - accuracy: 0.9384 - val_loss: 0.1374 - val_accuracy: 0.9610 - 7s/epoch - 16ms/step\n",
      "Epoch 31/200\n",
      "422/422 - 7s - loss: 0.2043 - accuracy: 0.9392 - val_loss: 0.1318 - val_accuracy: 0.9622 - 7s/epoch - 16ms/step\n",
      "Epoch 32/200\n",
      "422/422 - 7s - loss: 0.1961 - accuracy: 0.9419 - val_loss: 0.1271 - val_accuracy: 0.9643 - 7s/epoch - 16ms/step\n",
      "Epoch 33/200\n",
      "422/422 - 6s - loss: 0.1934 - accuracy: 0.9429 - val_loss: 0.1292 - val_accuracy: 0.9640 - 6s/epoch - 15ms/step\n",
      "Epoch 34/200\n",
      "422/422 - 7s - loss: 0.1888 - accuracy: 0.9432 - val_loss: 0.1208 - val_accuracy: 0.9657 - 7s/epoch - 16ms/step\n",
      "Epoch 35/200\n",
      "  >> Test@035 - loss: 0.1355 - acc: 0.9586\n",
      "422/422 - 7s - loss: 0.1851 - accuracy: 0.9447 - val_loss: 0.1168 - val_accuracy: 0.9662 - 7s/epoch - 16ms/step\n",
      "Epoch 36/200\n",
      "422/422 - 6s - loss: 0.1784 - accuracy: 0.9471 - val_loss: 0.1193 - val_accuracy: 0.9670 - 6s/epoch - 15ms/step\n",
      "Epoch 37/200\n",
      "422/422 - 7s - loss: 0.1785 - accuracy: 0.9460 - val_loss: 0.1156 - val_accuracy: 0.9667 - 7s/epoch - 16ms/step\n",
      "Epoch 38/200\n",
      "422/422 - 7s - loss: 0.1760 - accuracy: 0.9463 - val_loss: 0.1127 - val_accuracy: 0.9673 - 7s/epoch - 16ms/step\n",
      "Epoch 39/200\n",
      "422/422 - 7s - loss: 0.1708 - accuracy: 0.9495 - val_loss: 0.1058 - val_accuracy: 0.9702 - 7s/epoch - 16ms/step\n",
      "Epoch 40/200\n",
      "  >> Test@040 - loss: 0.1173 - acc: 0.9647\n",
      "422/422 - 7s - loss: 0.1677 - accuracy: 0.9497 - val_loss: 0.1022 - val_accuracy: 0.9710 - 7s/epoch - 16ms/step\n",
      "Epoch 41/200\n",
      "422/422 - 7s - loss: 0.1644 - accuracy: 0.9508 - val_loss: 0.1027 - val_accuracy: 0.9707 - 7s/epoch - 16ms/step\n",
      "Epoch 42/200\n",
      "422/422 - 7s - loss: 0.1627 - accuracy: 0.9509 - val_loss: 0.0998 - val_accuracy: 0.9722 - 7s/epoch - 16ms/step\n",
      "Epoch 43/200\n",
      "422/422 - 6s - loss: 0.1583 - accuracy: 0.9519 - val_loss: 0.0986 - val_accuracy: 0.9723 - 6s/epoch - 15ms/step\n",
      "Epoch 44/200\n",
      "422/422 - 7s - loss: 0.1548 - accuracy: 0.9523 - val_loss: 0.0985 - val_accuracy: 0.9718 - 7s/epoch - 16ms/step\n",
      "Epoch 45/200\n",
      "  >> Test@045 - loss: 0.1075 - acc: 0.9671\n",
      "422/422 - 7s - loss: 0.1566 - accuracy: 0.9539 - val_loss: 0.0920 - val_accuracy: 0.9757 - 7s/epoch - 17ms/step\n",
      "Epoch 46/200\n",
      "422/422 - 6s - loss: 0.1541 - accuracy: 0.9545 - val_loss: 0.0933 - val_accuracy: 0.9738 - 6s/epoch - 15ms/step\n",
      "Epoch 47/200\n",
      "422/422 - 6s - loss: 0.1506 - accuracy: 0.9551 - val_loss: 0.0941 - val_accuracy: 0.9737 - 6s/epoch - 15ms/step\n",
      "Epoch 48/200\n",
      "422/422 - 7s - loss: 0.1469 - accuracy: 0.9559 - val_loss: 0.0921 - val_accuracy: 0.9733 - 7s/epoch - 15ms/step\n",
      "Epoch 49/200\n",
      "422/422 - 7s - loss: 0.1454 - accuracy: 0.9565 - val_loss: 0.0956 - val_accuracy: 0.9743 - 7s/epoch - 16ms/step\n",
      "Epoch 50/200\n",
      "  >> Test@050 - loss: 0.1103 - acc: 0.9652\n",
      "422/422 - 7s - loss: 0.1457 - accuracy: 0.9559 - val_loss: 0.0953 - val_accuracy: 0.9740 - 7s/epoch - 16ms/step\n",
      "Epoch 51/200\n",
      "422/422 - 7s - loss: 0.1404 - accuracy: 0.9578 - val_loss: 0.0855 - val_accuracy: 0.9763 - 7s/epoch - 16ms/step\n",
      "Epoch 52/200\n",
      "422/422 - 7s - loss: 0.1434 - accuracy: 0.9571 - val_loss: 0.0855 - val_accuracy: 0.9757 - 7s/epoch - 16ms/step\n",
      "Epoch 53/200\n",
      "422/422 - 7s - loss: 0.1414 - accuracy: 0.9581 - val_loss: 0.0853 - val_accuracy: 0.9762 - 7s/epoch - 16ms/step\n",
      "Epoch 54/200\n",
      "422/422 - 7s - loss: 0.1391 - accuracy: 0.9573 - val_loss: 0.0829 - val_accuracy: 0.9783 - 7s/epoch - 16ms/step\n",
      "Epoch 55/200\n",
      "  >> Test@055 - loss: 0.0993 - acc: 0.9694\n",
      "422/422 - 7s - loss: 0.1391 - accuracy: 0.9582 - val_loss: 0.0868 - val_accuracy: 0.9763 - 7s/epoch - 17ms/step\n",
      "Epoch 56/200\n",
      "422/422 - 7s - loss: 0.1340 - accuracy: 0.9600 - val_loss: 0.0845 - val_accuracy: 0.9777 - 7s/epoch - 15ms/step\n",
      "Epoch 57/200\n",
      "422/422 - 7s - loss: 0.1347 - accuracy: 0.9598 - val_loss: 0.0808 - val_accuracy: 0.9778 - 7s/epoch - 16ms/step\n",
      "Epoch 58/200\n",
      "422/422 - 7s - loss: 0.1332 - accuracy: 0.9598 - val_loss: 0.0822 - val_accuracy: 0.9778 - 7s/epoch - 16ms/step\n",
      "Epoch 59/200\n",
      "422/422 - 7s - loss: 0.1306 - accuracy: 0.9611 - val_loss: 0.0806 - val_accuracy: 0.9778 - 7s/epoch - 16ms/step\n",
      "Epoch 60/200\n",
      "  >> Test@060 - loss: 0.0921 - acc: 0.9716\n",
      "422/422 - 7s - loss: 0.1321 - accuracy: 0.9605 - val_loss: 0.0818 - val_accuracy: 0.9777 - 7s/epoch - 16ms/step\n",
      "Epoch 61/200\n",
      "422/422 - 7s - loss: 0.1289 - accuracy: 0.9610 - val_loss: 0.0800 - val_accuracy: 0.9798 - 7s/epoch - 16ms/step\n",
      "Epoch 62/200\n",
      "422/422 - 7s - loss: 0.1279 - accuracy: 0.9616 - val_loss: 0.0823 - val_accuracy: 0.9768 - 7s/epoch - 16ms/step\n",
      "Epoch 63/200\n",
      "422/422 - 7s - loss: 0.1286 - accuracy: 0.9612 - val_loss: 0.0773 - val_accuracy: 0.9807 - 7s/epoch - 16ms/step\n",
      "Epoch 64/200\n",
      "422/422 - 7s - loss: 0.1256 - accuracy: 0.9628 - val_loss: 0.0768 - val_accuracy: 0.9802 - 7s/epoch - 16ms/step\n",
      "Epoch 65/200\n",
      "  >> Test@065 - loss: 0.0864 - acc: 0.9744\n",
      "422/422 - 7s - loss: 0.1269 - accuracy: 0.9629 - val_loss: 0.0756 - val_accuracy: 0.9795 - 7s/epoch - 17ms/step\n",
      "Epoch 66/200\n",
      "422/422 - 6s - loss: 0.1244 - accuracy: 0.9621 - val_loss: 0.0761 - val_accuracy: 0.9798 - 6s/epoch - 13ms/step\n",
      "Epoch 67/200\n",
      "422/422 - 5s - loss: 0.1239 - accuracy: 0.9629 - val_loss: 0.0759 - val_accuracy: 0.9785 - 5s/epoch - 12ms/step\n",
      "Epoch 68/200\n",
      "422/422 - 5s - loss: 0.1221 - accuracy: 0.9629 - val_loss: 0.0752 - val_accuracy: 0.9787 - 5s/epoch - 12ms/step\n",
      "Epoch 69/200\n",
      "422/422 - 5s - loss: 0.1229 - accuracy: 0.9626 - val_loss: 0.0792 - val_accuracy: 0.9782 - 5s/epoch - 12ms/step\n",
      "Epoch 70/200\n",
      "  >> Test@070 - loss: 0.0854 - acc: 0.9739\n",
      "422/422 - 6s - loss: 0.1213 - accuracy: 0.9633 - val_loss: 0.0750 - val_accuracy: 0.9793 - 6s/epoch - 13ms/step\n",
      "Epoch 71/200\n",
      "422/422 - 5s - loss: 0.1193 - accuracy: 0.9644 - val_loss: 0.0727 - val_accuracy: 0.9800 - 5s/epoch - 13ms/step\n",
      "Epoch 72/200\n",
      "422/422 - 5s - loss: 0.1195 - accuracy: 0.9636 - val_loss: 0.0766 - val_accuracy: 0.9785 - 5s/epoch - 12ms/step\n",
      "Epoch 73/200\n",
      "422/422 - 5s - loss: 0.1189 - accuracy: 0.9632 - val_loss: 0.0742 - val_accuracy: 0.9790 - 5s/epoch - 13ms/step\n",
      "Epoch 74/200\n",
      "422/422 - 5s - loss: 0.1182 - accuracy: 0.9644 - val_loss: 0.0754 - val_accuracy: 0.9780 - 5s/epoch - 12ms/step\n",
      "Epoch 75/200\n",
      "  >> Test@075 - loss: 0.0825 - acc: 0.9742\n",
      "422/422 - 6s - loss: 0.1149 - accuracy: 0.9643 - val_loss: 0.0724 - val_accuracy: 0.9803 - 6s/epoch - 13ms/step\n",
      "Epoch 76/200\n",
      "422/422 - 5s - loss: 0.1163 - accuracy: 0.9653 - val_loss: 0.0754 - val_accuracy: 0.9775 - 5s/epoch - 12ms/step\n",
      "Epoch 77/200\n",
      "422/422 - 5s - loss: 0.1148 - accuracy: 0.9662 - val_loss: 0.0722 - val_accuracy: 0.9808 - 5s/epoch - 12ms/step\n",
      "Epoch 78/200\n",
      "422/422 - 5s - loss: 0.1153 - accuracy: 0.9658 - val_loss: 0.0695 - val_accuracy: 0.9812 - 5s/epoch - 13ms/step\n",
      "Epoch 79/200\n",
      "422/422 - 5s - loss: 0.1154 - accuracy: 0.9648 - val_loss: 0.0694 - val_accuracy: 0.9825 - 5s/epoch - 12ms/step\n",
      "Epoch 80/200\n",
      "  >> Test@080 - loss: 0.0805 - acc: 0.9760\n",
      "422/422 - 5s - loss: 0.1119 - accuracy: 0.9660 - val_loss: 0.0690 - val_accuracy: 0.9813 - 5s/epoch - 13ms/step\n",
      "Epoch 81/200\n",
      "422/422 - 5s - loss: 0.1115 - accuracy: 0.9664 - val_loss: 0.0692 - val_accuracy: 0.9813 - 5s/epoch - 12ms/step\n",
      "Epoch 82/200\n",
      "422/422 - 5s - loss: 0.1106 - accuracy: 0.9669 - val_loss: 0.0680 - val_accuracy: 0.9823 - 5s/epoch - 12ms/step\n",
      "Epoch 83/200\n",
      "422/422 - 5s - loss: 0.1092 - accuracy: 0.9668 - val_loss: 0.0708 - val_accuracy: 0.9807 - 5s/epoch - 12ms/step\n",
      "Epoch 84/200\n",
      "422/422 - 5s - loss: 0.1115 - accuracy: 0.9662 - val_loss: 0.0720 - val_accuracy: 0.9795 - 5s/epoch - 13ms/step\n",
      "Epoch 85/200\n",
      "  >> Test@085 - loss: 0.0797 - acc: 0.9749\n",
      "422/422 - 5s - loss: 0.1090 - accuracy: 0.9670 - val_loss: 0.0706 - val_accuracy: 0.9808 - 5s/epoch - 12ms/step\n",
      "Epoch 86/200\n",
      "422/422 - 5s - loss: 0.1102 - accuracy: 0.9665 - val_loss: 0.0700 - val_accuracy: 0.9807 - 5s/epoch - 11ms/step\n",
      "Epoch 87/200\n",
      "422/422 - 5s - loss: 0.1078 - accuracy: 0.9676 - val_loss: 0.0720 - val_accuracy: 0.9802 - 5s/epoch - 12ms/step\n",
      "Epoch 88/200\n",
      "422/422 - 5s - loss: 0.1068 - accuracy: 0.9679 - val_loss: 0.0673 - val_accuracy: 0.9817 - 5s/epoch - 12ms/step\n",
      "Epoch 89/200\n",
      "422/422 - 5s - loss: 0.1085 - accuracy: 0.9678 - val_loss: 0.0682 - val_accuracy: 0.9820 - 5s/epoch - 12ms/step\n",
      "Epoch 90/200\n",
      "  >> Test@090 - loss: 0.0758 - acc: 0.9765\n",
      "422/422 - 6s - loss: 0.1069 - accuracy: 0.9671 - val_loss: 0.0668 - val_accuracy: 0.9810 - 6s/epoch - 13ms/step\n",
      "Epoch 91/200\n",
      "422/422 - 5s - loss: 0.1067 - accuracy: 0.9675 - val_loss: 0.0713 - val_accuracy: 0.9793 - 5s/epoch - 12ms/step\n",
      "Epoch 92/200\n",
      "422/422 - 5s - loss: 0.1081 - accuracy: 0.9680 - val_loss: 0.0732 - val_accuracy: 0.9790 - 5s/epoch - 12ms/step\n",
      "Epoch 93/200\n",
      "422/422 - 5s - loss: 0.1062 - accuracy: 0.9672 - val_loss: 0.0669 - val_accuracy: 0.9808 - 5s/epoch - 12ms/step\n",
      "Epoch 94/200\n",
      "422/422 - 5s - loss: 0.1062 - accuracy: 0.9688 - val_loss: 0.0677 - val_accuracy: 0.9808 - 5s/epoch - 12ms/step\n",
      "Epoch 95/200\n",
      "  >> Test@095 - loss: 0.0864 - acc: 0.9735\n",
      "422/422 - 5s - loss: 0.1054 - accuracy: 0.9681 - val_loss: 0.0764 - val_accuracy: 0.9782 - 5s/epoch - 13ms/step\n",
      "Epoch 96/200\n",
      "422/422 - 5s - loss: 0.1061 - accuracy: 0.9676 - val_loss: 0.0683 - val_accuracy: 0.9807 - 5s/epoch - 12ms/step\n",
      "Epoch 97/200\n",
      "422/422 - 5s - loss: 0.1030 - accuracy: 0.9694 - val_loss: 0.0668 - val_accuracy: 0.9817 - 5s/epoch - 12ms/step\n",
      "Epoch 98/200\n",
      "422/422 - 5s - loss: 0.1033 - accuracy: 0.9684 - val_loss: 0.0679 - val_accuracy: 0.9810 - 5s/epoch - 12ms/step\n",
      "Epoch 99/200\n",
      "422/422 - 5s - loss: 0.1023 - accuracy: 0.9683 - val_loss: 0.0663 - val_accuracy: 0.9815 - 5s/epoch - 12ms/step\n",
      "Epoch 100/200\n",
      "  >> Test@100 - loss: 0.0760 - acc: 0.9762\n",
      "422/422 - 5s - loss: 0.1022 - accuracy: 0.9683 - val_loss: 0.0689 - val_accuracy: 0.9818 - 5s/epoch - 13ms/step\n",
      "Epoch 101/200\n",
      "422/422 - 5s - loss: 0.1024 - accuracy: 0.9687 - val_loss: 0.0685 - val_accuracy: 0.9813 - 5s/epoch - 12ms/step\n",
      "Epoch 102/200\n",
      "422/422 - 5s - loss: 0.1016 - accuracy: 0.9691 - val_loss: 0.0653 - val_accuracy: 0.9827 - 5s/epoch - 12ms/step\n",
      "Epoch 103/200\n",
      "422/422 - 5s - loss: 0.1016 - accuracy: 0.9690 - val_loss: 0.0642 - val_accuracy: 0.9823 - 5s/epoch - 12ms/step\n",
      "Epoch 104/200\n",
      "422/422 - 5s - loss: 0.1019 - accuracy: 0.9691 - val_loss: 0.0660 - val_accuracy: 0.9818 - 5s/epoch - 12ms/step\n",
      "Epoch 105/200\n",
      "  >> Test@105 - loss: 0.0733 - acc: 0.9782\n",
      "422/422 - 6s - loss: 0.1018 - accuracy: 0.9698 - val_loss: 0.0667 - val_accuracy: 0.9815 - 6s/epoch - 13ms/step\n",
      "Epoch 106/200\n",
      "422/422 - 5s - loss: 0.1026 - accuracy: 0.9689 - val_loss: 0.0648 - val_accuracy: 0.9825 - 5s/epoch - 12ms/step\n",
      "Epoch 107/200\n",
      "422/422 - 5s - loss: 0.0993 - accuracy: 0.9693 - val_loss: 0.0668 - val_accuracy: 0.9810 - 5s/epoch - 12ms/step\n",
      "Epoch 108/200\n",
      "422/422 - 5s - loss: 0.1028 - accuracy: 0.9682 - val_loss: 0.0641 - val_accuracy: 0.9825 - 5s/epoch - 13ms/step\n",
      "Epoch 109/200\n",
      "422/422 - 5s - loss: 0.0985 - accuracy: 0.9704 - val_loss: 0.0713 - val_accuracy: 0.9808 - 5s/epoch - 12ms/step\n",
      "Epoch 110/200\n",
      "  >> Test@110 - loss: 0.0712 - acc: 0.9788\n",
      "422/422 - 5s - loss: 0.0985 - accuracy: 0.9706 - val_loss: 0.0620 - val_accuracy: 0.9823 - 5s/epoch - 13ms/step\n",
      "Epoch 111/200\n",
      "422/422 - 5s - loss: 0.0986 - accuracy: 0.9696 - val_loss: 0.0681 - val_accuracy: 0.9820 - 5s/epoch - 12ms/step\n",
      "Epoch 112/200\n",
      "422/422 - 5s - loss: 0.0968 - accuracy: 0.9700 - val_loss: 0.0631 - val_accuracy: 0.9833 - 5s/epoch - 12ms/step\n",
      "Epoch 113/200\n",
      "422/422 - 5s - loss: 0.0985 - accuracy: 0.9698 - val_loss: 0.0641 - val_accuracy: 0.9823 - 5s/epoch - 12ms/step\n",
      "Epoch 114/200\n",
      "422/422 - 5s - loss: 0.0969 - accuracy: 0.9706 - val_loss: 0.0647 - val_accuracy: 0.9823 - 5s/epoch - 12ms/step\n",
      "Epoch 115/200\n",
      "  >> Test@115 - loss: 0.0713 - acc: 0.9786\n",
      "422/422 - 5s - loss: 0.0982 - accuracy: 0.9694 - val_loss: 0.0641 - val_accuracy: 0.9815 - 5s/epoch - 13ms/step\n",
      "Epoch 116/200\n",
      "422/422 - 5s - loss: 0.0980 - accuracy: 0.9701 - val_loss: 0.0678 - val_accuracy: 0.9822 - 5s/epoch - 12ms/step\n",
      "Epoch 117/200\n",
      "422/422 - 5s - loss: 0.0971 - accuracy: 0.9703 - val_loss: 0.0646 - val_accuracy: 0.9830 - 5s/epoch - 12ms/step\n",
      "Epoch 118/200\n",
      "422/422 - 5s - loss: 0.0980 - accuracy: 0.9706 - val_loss: 0.0646 - val_accuracy: 0.9832 - 5s/epoch - 12ms/step\n",
      "Epoch 119/200\n",
      "422/422 - 5s - loss: 0.0953 - accuracy: 0.9715 - val_loss: 0.0623 - val_accuracy: 0.9822 - 5s/epoch - 12ms/step\n",
      "Epoch 120/200\n",
      "  >> Test@120 - loss: 0.0672 - acc: 0.9787\n",
      "422/422 - 5s - loss: 0.0950 - accuracy: 0.9709 - val_loss: 0.0617 - val_accuracy: 0.9823 - 5s/epoch - 13ms/step\n",
      "Epoch 121/200\n",
      "422/422 - 5s - loss: 0.0978 - accuracy: 0.9701 - val_loss: 0.0711 - val_accuracy: 0.9795 - 5s/epoch - 12ms/step\n",
      "Epoch 122/200\n",
      "422/422 - 5s - loss: 0.0950 - accuracy: 0.9713 - val_loss: 0.0661 - val_accuracy: 0.9817 - 5s/epoch - 12ms/step\n",
      "Epoch 123/200\n",
      "422/422 - 5s - loss: 0.0953 - accuracy: 0.9715 - val_loss: 0.0655 - val_accuracy: 0.9813 - 5s/epoch - 12ms/step\n",
      "Epoch 124/200\n",
      "422/422 - 5s - loss: 0.0949 - accuracy: 0.9713 - val_loss: 0.0621 - val_accuracy: 0.9835 - 5s/epoch - 12ms/step\n",
      "Epoch 125/200\n",
      "  >> Test@125 - loss: 0.0706 - acc: 0.9788\n",
      "422/422 - 5s - loss: 0.0949 - accuracy: 0.9704 - val_loss: 0.0643 - val_accuracy: 0.9833 - 5s/epoch - 13ms/step\n",
      "Epoch 126/200\n",
      "422/422 - 5s - loss: 0.0942 - accuracy: 0.9712 - val_loss: 0.0617 - val_accuracy: 0.9833 - 5s/epoch - 12ms/step\n",
      "Epoch 127/200\n",
      "422/422 - 5s - loss: 0.0953 - accuracy: 0.9711 - val_loss: 0.0603 - val_accuracy: 0.9830 - 5s/epoch - 12ms/step\n",
      "Epoch 128/200\n",
      "422/422 - 5s - loss: 0.0951 - accuracy: 0.9707 - val_loss: 0.0611 - val_accuracy: 0.9825 - 5s/epoch - 12ms/step\n",
      "Epoch 129/200\n",
      "422/422 - 5s - loss: 0.0942 - accuracy: 0.9715 - val_loss: 0.0621 - val_accuracy: 0.9830 - 5s/epoch - 12ms/step\n",
      "Epoch 130/200\n",
      "  >> Test@130 - loss: 0.0683 - acc: 0.9793\n",
      "422/422 - 6s - loss: 0.0937 - accuracy: 0.9712 - val_loss: 0.0608 - val_accuracy: 0.9835 - 6s/epoch - 13ms/step\n",
      "Epoch 131/200\n",
      "422/422 - 5s - loss: 0.0951 - accuracy: 0.9704 - val_loss: 0.0616 - val_accuracy: 0.9830 - 5s/epoch - 12ms/step\n",
      "Epoch 132/200\n",
      "422/422 - 5s - loss: 0.0925 - accuracy: 0.9711 - val_loss: 0.0673 - val_accuracy: 0.9812 - 5s/epoch - 11ms/step\n",
      "Epoch 133/200\n",
      "422/422 - 5s - loss: 0.0930 - accuracy: 0.9719 - val_loss: 0.0616 - val_accuracy: 0.9832 - 5s/epoch - 12ms/step\n",
      "Epoch 134/200\n",
      "422/422 - 5s - loss: 0.0933 - accuracy: 0.9714 - val_loss: 0.0601 - val_accuracy: 0.9842 - 5s/epoch - 12ms/step\n",
      "Epoch 135/200\n",
      "  >> Test@135 - loss: 0.0659 - acc: 0.9801\n",
      "422/422 - 6s - loss: 0.0946 - accuracy: 0.9714 - val_loss: 0.0605 - val_accuracy: 0.9837 - 6s/epoch - 13ms/step\n",
      "Epoch 136/200\n",
      "422/422 - 5s - loss: 0.0924 - accuracy: 0.9716 - val_loss: 0.0634 - val_accuracy: 0.9820 - 5s/epoch - 12ms/step\n",
      "Epoch 137/200\n",
      "422/422 - 5s - loss: 0.0922 - accuracy: 0.9719 - val_loss: 0.0608 - val_accuracy: 0.9835 - 5s/epoch - 12ms/step\n",
      "Epoch 138/200\n",
      "422/422 - 5s - loss: 0.0918 - accuracy: 0.9715 - val_loss: 0.0623 - val_accuracy: 0.9833 - 5s/epoch - 12ms/step\n",
      "Epoch 139/200\n",
      "422/422 - 5s - loss: 0.0920 - accuracy: 0.9723 - val_loss: 0.0620 - val_accuracy: 0.9832 - 5s/epoch - 12ms/step\n",
      "Epoch 140/200\n",
      "  >> Test@140 - loss: 0.0638 - acc: 0.9805\n",
      "422/422 - 5s - loss: 0.0911 - accuracy: 0.9729 - val_loss: 0.0583 - val_accuracy: 0.9838 - 5s/epoch - 13ms/step\n",
      "Epoch 141/200\n",
      "422/422 - 5s - loss: 0.0899 - accuracy: 0.9733 - val_loss: 0.0627 - val_accuracy: 0.9830 - 5s/epoch - 12ms/step\n",
      "Epoch 142/200\n",
      "422/422 - 5s - loss: 0.0915 - accuracy: 0.9714 - val_loss: 0.0597 - val_accuracy: 0.9842 - 5s/epoch - 13ms/step\n",
      "Epoch 143/200\n",
      "422/422 - 5s - loss: 0.0945 - accuracy: 0.9715 - val_loss: 0.0622 - val_accuracy: 0.9832 - 5s/epoch - 12ms/step\n",
      "Epoch 144/200\n",
      "422/422 - 5s - loss: 0.0889 - accuracy: 0.9732 - val_loss: 0.0649 - val_accuracy: 0.9825 - 5s/epoch - 12ms/step\n",
      "Epoch 145/200\n",
      "  >> Test@145 - loss: 0.0671 - acc: 0.9799\n",
      "422/422 - 5s - loss: 0.0914 - accuracy: 0.9725 - val_loss: 0.0605 - val_accuracy: 0.9833 - 5s/epoch - 13ms/step\n",
      "Epoch 146/200\n",
      "422/422 - 5s - loss: 0.0897 - accuracy: 0.9722 - val_loss: 0.0593 - val_accuracy: 0.9847 - 5s/epoch - 12ms/step\n",
      "Epoch 147/200\n",
      "422/422 - 5s - loss: 0.0914 - accuracy: 0.9719 - val_loss: 0.0606 - val_accuracy: 0.9830 - 5s/epoch - 12ms/step\n",
      "Epoch 148/200\n",
      "422/422 - 5s - loss: 0.0895 - accuracy: 0.9726 - val_loss: 0.0600 - val_accuracy: 0.9838 - 5s/epoch - 12ms/step\n",
      "Epoch 149/200\n",
      "422/422 - 5s - loss: 0.0891 - accuracy: 0.9727 - val_loss: 0.0591 - val_accuracy: 0.9840 - 5s/epoch - 12ms/step\n",
      "Epoch 150/200\n",
      "  >> Test@150 - loss: 0.0644 - acc: 0.9806\n",
      "422/422 - 6s - loss: 0.0907 - accuracy: 0.9718 - val_loss: 0.0592 - val_accuracy: 0.9843 - 6s/epoch - 13ms/step\n",
      "Epoch 151/200\n",
      "422/422 - 5s - loss: 0.0912 - accuracy: 0.9717 - val_loss: 0.0594 - val_accuracy: 0.9827 - 5s/epoch - 12ms/step\n",
      "Epoch 152/200\n",
      "422/422 - 5s - loss: 0.0893 - accuracy: 0.9730 - val_loss: 0.0639 - val_accuracy: 0.9825 - 5s/epoch - 12ms/step\n",
      "Epoch 153/200\n",
      "422/422 - 5s - loss: 0.0893 - accuracy: 0.9718 - val_loss: 0.0583 - val_accuracy: 0.9842 - 5s/epoch - 12ms/step\n",
      "Epoch 154/200\n",
      "422/422 - 5s - loss: 0.0900 - accuracy: 0.9719 - val_loss: 0.0588 - val_accuracy: 0.9837 - 5s/epoch - 12ms/step\n",
      "Epoch 155/200\n",
      "  >> Test@155 - loss: 0.0641 - acc: 0.9801\n",
      "422/422 - 5s - loss: 0.0902 - accuracy: 0.9722 - val_loss: 0.0593 - val_accuracy: 0.9842 - 5s/epoch - 13ms/step\n",
      "Epoch 156/200\n",
      "422/422 - 5s - loss: 0.0921 - accuracy: 0.9718 - val_loss: 0.0588 - val_accuracy: 0.9843 - 5s/epoch - 12ms/step\n",
      "Epoch 157/200\n",
      "422/422 - 5s - loss: 0.0894 - accuracy: 0.9727 - val_loss: 0.0588 - val_accuracy: 0.9842 - 5s/epoch - 12ms/step\n",
      "Epoch 158/200\n",
      "422/422 - 5s - loss: 0.0873 - accuracy: 0.9738 - val_loss: 0.0588 - val_accuracy: 0.9842 - 5s/epoch - 12ms/step\n",
      "Epoch 159/200\n",
      "422/422 - 5s - loss: 0.0876 - accuracy: 0.9735 - val_loss: 0.0575 - val_accuracy: 0.9847 - 5s/epoch - 12ms/step\n",
      "Epoch 160/200\n",
      "  >> Test@160 - loss: 0.0641 - acc: 0.9798\n",
      "422/422 - 5s - loss: 0.0864 - accuracy: 0.9742 - val_loss: 0.0593 - val_accuracy: 0.9827 - 5s/epoch - 13ms/step\n",
      "Epoch 161/200\n",
      "422/422 - 5s - loss: 0.0882 - accuracy: 0.9728 - val_loss: 0.0599 - val_accuracy: 0.9840 - 5s/epoch - 12ms/step\n",
      "Epoch 162/200\n",
      "422/422 - 5s - loss: 0.0870 - accuracy: 0.9741 - val_loss: 0.0580 - val_accuracy: 0.9843 - 5s/epoch - 12ms/step\n",
      "Epoch 163/200\n",
      "422/422 - 5s - loss: 0.0891 - accuracy: 0.9725 - val_loss: 0.0629 - val_accuracy: 0.9830 - 5s/epoch - 12ms/step\n",
      "Epoch 164/200\n",
      "422/422 - 5s - loss: 0.0889 - accuracy: 0.9727 - val_loss: 0.0632 - val_accuracy: 0.9823 - 5s/epoch - 12ms/step\n",
      "Epoch 165/200\n",
      "  >> Test@165 - loss: 0.0611 - acc: 0.9805\n",
      "422/422 - 5s - loss: 0.0880 - accuracy: 0.9735 - val_loss: 0.0569 - val_accuracy: 0.9832 - 5s/epoch - 13ms/step\n",
      "Epoch 166/200\n",
      "422/422 - 5s - loss: 0.0876 - accuracy: 0.9734 - val_loss: 0.0573 - val_accuracy: 0.9833 - 5s/epoch - 12ms/step\n",
      "Epoch 167/200\n",
      "422/422 - 5s - loss: 0.0875 - accuracy: 0.9733 - val_loss: 0.0567 - val_accuracy: 0.9842 - 5s/epoch - 12ms/step\n",
      "Epoch 168/200\n",
      "422/422 - 5s - loss: 0.0908 - accuracy: 0.9720 - val_loss: 0.0575 - val_accuracy: 0.9847 - 5s/epoch - 12ms/step\n",
      "Epoch 169/200\n",
      "422/422 - 5s - loss: 0.0874 - accuracy: 0.9735 - val_loss: 0.0628 - val_accuracy: 0.9833 - 5s/epoch - 12ms/step\n",
      "Epoch 170/200\n",
      "  >> Test@170 - loss: 0.0639 - acc: 0.9806\n",
      "422/422 - 5s - loss: 0.0877 - accuracy: 0.9732 - val_loss: 0.0578 - val_accuracy: 0.9830 - 5s/epoch - 13ms/step\n",
      "Epoch 171/200\n",
      "422/422 - 5s - loss: 0.0871 - accuracy: 0.9739 - val_loss: 0.0565 - val_accuracy: 0.9833 - 5s/epoch - 12ms/step\n",
      "Epoch 172/200\n",
      "422/422 - 5s - loss: 0.0861 - accuracy: 0.9731 - val_loss: 0.0572 - val_accuracy: 0.9847 - 5s/epoch - 12ms/step\n",
      "Epoch 173/200\n",
      "422/422 - 5s - loss: 0.0874 - accuracy: 0.9730 - val_loss: 0.0568 - val_accuracy: 0.9840 - 5s/epoch - 12ms/step\n",
      "Epoch 174/200\n",
      "422/422 - 5s - loss: 0.0870 - accuracy: 0.9729 - val_loss: 0.0575 - val_accuracy: 0.9833 - 5s/epoch - 12ms/step\n",
      "Epoch 175/200\n",
      "  >> Test@175 - loss: 0.0642 - acc: 0.9805\n",
      "422/422 - 6s - loss: 0.0871 - accuracy: 0.9735 - val_loss: 0.0582 - val_accuracy: 0.9838 - 6s/epoch - 13ms/step\n",
      "Epoch 176/200\n",
      "422/422 - 5s - loss: 0.0887 - accuracy: 0.9724 - val_loss: 0.0564 - val_accuracy: 0.9837 - 5s/epoch - 12ms/step\n",
      "Epoch 177/200\n",
      "422/422 - 5s - loss: 0.0878 - accuracy: 0.9735 - val_loss: 0.0579 - val_accuracy: 0.9833 - 5s/epoch - 12ms/step\n",
      "Epoch 178/200\n",
      "422/422 - 5s - loss: 0.0879 - accuracy: 0.9724 - val_loss: 0.0573 - val_accuracy: 0.9833 - 5s/epoch - 12ms/step\n",
      "Epoch 179/200\n",
      "422/422 - 5s - loss: 0.0847 - accuracy: 0.9736 - val_loss: 0.0560 - val_accuracy: 0.9852 - 5s/epoch - 12ms/step\n",
      "Epoch 180/200\n",
      "  >> Test@180 - loss: 0.0621 - acc: 0.9810\n",
      "422/422 - 5s - loss: 0.0866 - accuracy: 0.9736 - val_loss: 0.0580 - val_accuracy: 0.9832 - 5s/epoch - 13ms/step\n",
      "Epoch 181/200\n",
      "422/422 - 5s - loss: 0.0876 - accuracy: 0.9727 - val_loss: 0.0558 - val_accuracy: 0.9845 - 5s/epoch - 12ms/step\n",
      "Epoch 182/200\n",
      "422/422 - 5s - loss: 0.0859 - accuracy: 0.9735 - val_loss: 0.0551 - val_accuracy: 0.9853 - 5s/epoch - 12ms/step\n",
      "Epoch 183/200\n",
      "422/422 - 5s - loss: 0.0851 - accuracy: 0.9741 - val_loss: 0.0579 - val_accuracy: 0.9843 - 5s/epoch - 12ms/step\n",
      "Epoch 184/200\n",
      "422/422 - 5s - loss: 0.0883 - accuracy: 0.9732 - val_loss: 0.0634 - val_accuracy: 0.9825 - 5s/epoch - 12ms/step\n",
      "Epoch 185/200\n",
      "  >> Test@185 - loss: 0.0629 - acc: 0.9797\n",
      "422/422 - 6s - loss: 0.0843 - accuracy: 0.9737 - val_loss: 0.0577 - val_accuracy: 0.9837 - 6s/epoch - 13ms/step\n",
      "Epoch 186/200\n",
      "422/422 - 5s - loss: 0.0868 - accuracy: 0.9733 - val_loss: 0.0623 - val_accuracy: 0.9823 - 5s/epoch - 12ms/step\n",
      "Epoch 187/200\n",
      "422/422 - 5s - loss: 0.0867 - accuracy: 0.9741 - val_loss: 0.0608 - val_accuracy: 0.9837 - 5s/epoch - 12ms/step\n",
      "Epoch 188/200\n",
      "422/422 - 5s - loss: 0.0854 - accuracy: 0.9736 - val_loss: 0.0593 - val_accuracy: 0.9825 - 5s/epoch - 12ms/step\n",
      "Epoch 189/200\n",
      "422/422 - 5s - loss: 0.0865 - accuracy: 0.9734 - val_loss: 0.0616 - val_accuracy: 0.9830 - 5s/epoch - 13ms/step\n",
      "Epoch 190/200\n",
      "  >> Test@190 - loss: 0.0660 - acc: 0.9801\n",
      "422/422 - 5s - loss: 0.0866 - accuracy: 0.9736 - val_loss: 0.0630 - val_accuracy: 0.9807 - 5s/epoch - 13ms/step\n",
      "Epoch 191/200\n",
      "422/422 - 5s - loss: 0.0860 - accuracy: 0.9736 - val_loss: 0.0566 - val_accuracy: 0.9840 - 5s/epoch - 12ms/step\n",
      "Epoch 192/200\n",
      "422/422 - 5s - loss: 0.0865 - accuracy: 0.9731 - val_loss: 0.0590 - val_accuracy: 0.9828 - 5s/epoch - 12ms/step\n",
      "Epoch 193/200\n",
      "422/422 - 5s - loss: 0.0861 - accuracy: 0.9732 - val_loss: 0.0570 - val_accuracy: 0.9837 - 5s/epoch - 12ms/step\n",
      "Epoch 194/200\n",
      "422/422 - 5s - loss: 0.0828 - accuracy: 0.9746 - val_loss: 0.0570 - val_accuracy: 0.9845 - 5s/epoch - 12ms/step\n",
      "Epoch 195/200\n",
      "  >> Test@195 - loss: 0.0621 - acc: 0.9797\n",
      "422/422 - 5s - loss: 0.0864 - accuracy: 0.9739 - val_loss: 0.0550 - val_accuracy: 0.9842 - 5s/epoch - 12ms/step\n",
      "Epoch 196/200\n",
      "422/422 - 5s - loss: 0.0883 - accuracy: 0.9729 - val_loss: 0.0597 - val_accuracy: 0.9828 - 5s/epoch - 12ms/step\n",
      "Epoch 197/200\n",
      "422/422 - 5s - loss: 0.0840 - accuracy: 0.9745 - val_loss: 0.0688 - val_accuracy: 0.9813 - 5s/epoch - 12ms/step\n",
      "Epoch 198/200\n",
      "422/422 - 5s - loss: 0.0869 - accuracy: 0.9732 - val_loss: 0.0557 - val_accuracy: 0.9838 - 5s/epoch - 13ms/step\n",
      "Epoch 199/200\n",
      "422/422 - 5s - loss: 0.0850 - accuracy: 0.9744 - val_loss: 0.0605 - val_accuracy: 0.9833 - 5s/epoch - 12ms/step\n",
      "Epoch 200/200\n",
      "  >> Test@200 - loss: 0.0593 - acc: 0.9808\n",
      "422/422 - 5s - loss: 0.0844 - accuracy: 0.9739 - val_loss: 0.0572 - val_accuracy: 0.9837 - 5s/epoch - 13ms/step\n",
      "Done: FixProb | Final TestAcc=0.9808 | nW=128725\n",
      "\n",
      "\n",
      " Training: SET (RAND)\n",
      "Epoch 1/200\n",
      "422/422 - 8s - loss: 2.2174 - accuracy: 0.3199 - val_loss: 2.0208 - val_accuracy: 0.6677 - 8s/epoch - 19ms/step\n",
      "Epoch 2/200\n",
      "422/422 - 5s - loss: 1.2790 - accuracy: 0.6867 - val_loss: 0.6373 - val_accuracy: 0.8335 - 5s/epoch - 12ms/step\n",
      "Epoch 3/200\n",
      "422/422 - 5s - loss: 0.6002 - accuracy: 0.8219 - val_loss: 0.3967 - val_accuracy: 0.8933 - 5s/epoch - 12ms/step\n",
      "Epoch 4/200\n",
      "422/422 - 5s - loss: 0.4660 - accuracy: 0.8640 - val_loss: 0.3302 - val_accuracy: 0.9100 - 5s/epoch - 12ms/step\n",
      "Epoch 5/200\n",
      "  >> Test@005 - loss: 0.3499 - acc: 0.9025\n",
      "422/422 - 5s - loss: 0.4182 - accuracy: 0.8782 - val_loss: 0.3022 - val_accuracy: 0.9167 - 5s/epoch - 13ms/step\n",
      "Epoch 6/200\n",
      "422/422 - 5s - loss: 0.3895 - accuracy: 0.8876 - val_loss: 0.2872 - val_accuracy: 0.9188 - 5s/epoch - 12ms/step\n",
      "Epoch 7/200\n",
      "422/422 - 5s - loss: 0.3738 - accuracy: 0.8919 - val_loss: 0.2726 - val_accuracy: 0.9230 - 5s/epoch - 12ms/step\n",
      "Epoch 8/200\n",
      "422/422 - 5s - loss: 0.3627 - accuracy: 0.8962 - val_loss: 0.2664 - val_accuracy: 0.9233 - 5s/epoch - 11ms/step\n",
      "Epoch 9/200\n",
      "422/422 - 5s - loss: 0.3549 - accuracy: 0.8976 - val_loss: 0.2602 - val_accuracy: 0.9263 - 5s/epoch - 12ms/step\n",
      "Epoch 10/200\n",
      "  >> Test@010 - loss: 0.3045 - acc: 0.9121\n",
      "422/422 - 6s - loss: 0.3464 - accuracy: 0.8997 - val_loss: 0.2562 - val_accuracy: 0.9295 - 6s/epoch - 14ms/step\n",
      "Epoch 11/200\n",
      "422/422 - 5s - loss: 0.3405 - accuracy: 0.9022 - val_loss: 0.2534 - val_accuracy: 0.9282 - 5s/epoch - 13ms/step\n",
      "Epoch 12/200\n",
      "422/422 - 5s - loss: 0.3325 - accuracy: 0.9037 - val_loss: 0.2456 - val_accuracy: 0.9300 - 5s/epoch - 13ms/step\n",
      "Epoch 13/200\n",
      "422/422 - 5s - loss: 0.3236 - accuracy: 0.9076 - val_loss: 0.2419 - val_accuracy: 0.9320 - 5s/epoch - 13ms/step\n",
      "Epoch 14/200\n",
      "422/422 - 5s - loss: 0.3125 - accuracy: 0.9090 - val_loss: 0.2298 - val_accuracy: 0.9325 - 5s/epoch - 13ms/step\n",
      "Epoch 15/200\n",
      "  >> Test@015 - loss: 0.2545 - acc: 0.9237\n",
      "422/422 - 6s - loss: 0.2997 - accuracy: 0.9113 - val_loss: 0.2164 - val_accuracy: 0.9368 - 6s/epoch - 13ms/step\n",
      "Epoch 16/200\n",
      "422/422 - 5s - loss: 0.2839 - accuracy: 0.9150 - val_loss: 0.2041 - val_accuracy: 0.9417 - 5s/epoch - 13ms/step\n",
      "Epoch 17/200\n",
      "422/422 - 5s - loss: 0.2675 - accuracy: 0.9209 - val_loss: 0.1865 - val_accuracy: 0.9463 - 5s/epoch - 13ms/step\n",
      "Epoch 18/200\n",
      "422/422 - 6s - loss: 0.2496 - accuracy: 0.9259 - val_loss: 0.1738 - val_accuracy: 0.9507 - 6s/epoch - 13ms/step\n",
      "Epoch 19/200\n",
      "422/422 - 5s - loss: 0.2350 - accuracy: 0.9310 - val_loss: 0.1593 - val_accuracy: 0.9558 - 5s/epoch - 13ms/step\n",
      "Epoch 20/200\n",
      "  >> Test@020 - loss: 0.1815 - acc: 0.9443\n",
      "422/422 - 6s - loss: 0.2220 - accuracy: 0.9335 - val_loss: 0.1456 - val_accuracy: 0.9595 - 6s/epoch - 14ms/step\n",
      "Epoch 21/200\n",
      "422/422 - 5s - loss: 0.2106 - accuracy: 0.9367 - val_loss: 0.1397 - val_accuracy: 0.9610 - 5s/epoch - 13ms/step\n",
      "Epoch 22/200\n",
      "422/422 - 6s - loss: 0.2010 - accuracy: 0.9396 - val_loss: 0.1349 - val_accuracy: 0.9617 - 6s/epoch - 13ms/step\n",
      "Epoch 23/200\n",
      "422/422 - 5s - loss: 0.1885 - accuracy: 0.9440 - val_loss: 0.1213 - val_accuracy: 0.9660 - 5s/epoch - 12ms/step\n",
      "Epoch 24/200\n",
      "422/422 - 5s - loss: 0.1803 - accuracy: 0.9464 - val_loss: 0.1191 - val_accuracy: 0.9650 - 5s/epoch - 13ms/step\n",
      "Epoch 25/200\n",
      "  >> Test@025 - loss: 0.1306 - acc: 0.9601\n",
      "422/422 - 6s - loss: 0.1725 - accuracy: 0.9478 - val_loss: 0.1092 - val_accuracy: 0.9690 - 6s/epoch - 14ms/step\n",
      "Epoch 26/200\n",
      "422/422 - 5s - loss: 0.1664 - accuracy: 0.9501 - val_loss: 0.1076 - val_accuracy: 0.9690 - 5s/epoch - 13ms/step\n",
      "Epoch 27/200\n",
      "422/422 - 5s - loss: 0.1601 - accuracy: 0.9523 - val_loss: 0.1066 - val_accuracy: 0.9693 - 5s/epoch - 13ms/step\n",
      "Epoch 28/200\n",
      "422/422 - 5s - loss: 0.1542 - accuracy: 0.9539 - val_loss: 0.0994 - val_accuracy: 0.9725 - 5s/epoch - 13ms/step\n",
      "Epoch 29/200\n",
      "422/422 - 6s - loss: 0.1479 - accuracy: 0.9554 - val_loss: 0.0964 - val_accuracy: 0.9723 - 6s/epoch - 13ms/step\n",
      "Epoch 30/200\n",
      "  >> Test@030 - loss: 0.1136 - acc: 0.9654\n",
      "422/422 - 6s - loss: 0.1456 - accuracy: 0.9555 - val_loss: 0.0944 - val_accuracy: 0.9728 - 6s/epoch - 14ms/step\n",
      "Epoch 31/200\n",
      "422/422 - 6s - loss: 0.1426 - accuracy: 0.9571 - val_loss: 0.0903 - val_accuracy: 0.9758 - 6s/epoch - 13ms/step\n",
      "Epoch 32/200\n",
      "422/422 - 5s - loss: 0.1371 - accuracy: 0.9586 - val_loss: 0.0866 - val_accuracy: 0.9765 - 5s/epoch - 13ms/step\n",
      "Epoch 33/200\n",
      "422/422 - 6s - loss: 0.1367 - accuracy: 0.9594 - val_loss: 0.0889 - val_accuracy: 0.9740 - 6s/epoch - 13ms/step\n",
      "Epoch 34/200\n",
      "422/422 - 6s - loss: 0.1304 - accuracy: 0.9616 - val_loss: 0.0882 - val_accuracy: 0.9760 - 6s/epoch - 13ms/step\n",
      "Epoch 35/200\n",
      "  >> Test@035 - loss: 0.0951 - acc: 0.9720\n",
      "422/422 - 6s - loss: 0.1279 - accuracy: 0.9622 - val_loss: 0.0815 - val_accuracy: 0.9777 - 6s/epoch - 14ms/step\n",
      "Epoch 36/200\n",
      "422/422 - 6s - loss: 0.1260 - accuracy: 0.9625 - val_loss: 0.0863 - val_accuracy: 0.9750 - 6s/epoch - 13ms/step\n",
      "Epoch 37/200\n",
      "422/422 - 6s - loss: 0.1252 - accuracy: 0.9625 - val_loss: 0.0787 - val_accuracy: 0.9788 - 6s/epoch - 13ms/step\n",
      "Epoch 38/200\n",
      "422/422 - 5s - loss: 0.1178 - accuracy: 0.9653 - val_loss: 0.0827 - val_accuracy: 0.9773 - 5s/epoch - 13ms/step\n",
      "Epoch 39/200\n",
      "422/422 - 5s - loss: 0.1174 - accuracy: 0.9645 - val_loss: 0.0775 - val_accuracy: 0.9790 - 5s/epoch - 13ms/step\n",
      "Epoch 40/200\n",
      "  >> Test@040 - loss: 0.0922 - acc: 0.9714\n",
      "422/422 - 6s - loss: 0.1180 - accuracy: 0.9651 - val_loss: 0.0737 - val_accuracy: 0.9807 - 6s/epoch - 13ms/step\n",
      "Epoch 41/200\n",
      "422/422 - 6s - loss: 0.1149 - accuracy: 0.9654 - val_loss: 0.0754 - val_accuracy: 0.9783 - 6s/epoch - 13ms/step\n",
      "Epoch 42/200\n",
      "422/422 - 5s - loss: 0.1123 - accuracy: 0.9667 - val_loss: 0.0726 - val_accuracy: 0.9813 - 5s/epoch - 13ms/step\n",
      "Epoch 43/200\n",
      "422/422 - 6s - loss: 0.1103 - accuracy: 0.9664 - val_loss: 0.0742 - val_accuracy: 0.9800 - 6s/epoch - 13ms/step\n",
      "Epoch 44/200\n",
      "422/422 - 6s - loss: 0.1099 - accuracy: 0.9667 - val_loss: 0.0732 - val_accuracy: 0.9797 - 6s/epoch - 13ms/step\n",
      "Epoch 45/200\n",
      "  >> Test@045 - loss: 0.0898 - acc: 0.9712\n",
      "422/422 - 6s - loss: 0.1081 - accuracy: 0.9673 - val_loss: 0.0750 - val_accuracy: 0.9805 - 6s/epoch - 13ms/step\n",
      "Epoch 46/200\n",
      "422/422 - 6s - loss: 0.1058 - accuracy: 0.9684 - val_loss: 0.0733 - val_accuracy: 0.9803 - 6s/epoch - 13ms/step\n",
      "Epoch 47/200\n",
      "422/422 - 6s - loss: 0.1059 - accuracy: 0.9673 - val_loss: 0.0736 - val_accuracy: 0.9795 - 6s/epoch - 13ms/step\n",
      "Epoch 48/200\n",
      "422/422 - 5s - loss: 0.1029 - accuracy: 0.9691 - val_loss: 0.0729 - val_accuracy: 0.9807 - 5s/epoch - 13ms/step\n",
      "Epoch 49/200\n",
      "422/422 - 5s - loss: 0.1036 - accuracy: 0.9684 - val_loss: 0.0703 - val_accuracy: 0.9810 - 5s/epoch - 13ms/step\n",
      "Epoch 50/200\n",
      "  >> Test@050 - loss: 0.0800 - acc: 0.9750\n",
      "422/422 - 6s - loss: 0.1019 - accuracy: 0.9694 - val_loss: 0.0685 - val_accuracy: 0.9805 - 6s/epoch - 14ms/step\n",
      "Epoch 51/200\n",
      "422/422 - 6s - loss: 0.0998 - accuracy: 0.9704 - val_loss: 0.0661 - val_accuracy: 0.9812 - 6s/epoch - 13ms/step\n",
      "Epoch 52/200\n",
      "422/422 - 6s - loss: 0.1005 - accuracy: 0.9690 - val_loss: 0.0669 - val_accuracy: 0.9812 - 6s/epoch - 13ms/step\n",
      "Epoch 53/200\n",
      "422/422 - 6s - loss: 0.0974 - accuracy: 0.9706 - val_loss: 0.0655 - val_accuracy: 0.9812 - 6s/epoch - 13ms/step\n",
      "Epoch 54/200\n",
      "422/422 - 5s - loss: 0.0967 - accuracy: 0.9701 - val_loss: 0.0648 - val_accuracy: 0.9822 - 5s/epoch - 13ms/step\n",
      "Epoch 55/200\n",
      "  >> Test@055 - loss: 0.0838 - acc: 0.9743\n",
      "422/422 - 6s - loss: 0.0957 - accuracy: 0.9707 - val_loss: 0.0695 - val_accuracy: 0.9810 - 6s/epoch - 14ms/step\n",
      "Epoch 56/200\n",
      "422/422 - 5s - loss: 0.0958 - accuracy: 0.9705 - val_loss: 0.0690 - val_accuracy: 0.9818 - 5s/epoch - 13ms/step\n",
      "Epoch 57/200\n",
      "422/422 - 5s - loss: 0.0944 - accuracy: 0.9711 - val_loss: 0.0631 - val_accuracy: 0.9815 - 5s/epoch - 13ms/step\n",
      "Epoch 58/200\n",
      "422/422 - 5s - loss: 0.0937 - accuracy: 0.9713 - val_loss: 0.0680 - val_accuracy: 0.9813 - 5s/epoch - 13ms/step\n",
      "Epoch 59/200\n",
      "422/422 - 6s - loss: 0.0918 - accuracy: 0.9719 - val_loss: 0.0631 - val_accuracy: 0.9838 - 6s/epoch - 13ms/step\n",
      "Epoch 60/200\n",
      "  >> Test@060 - loss: 0.0741 - acc: 0.9780\n",
      "422/422 - 6s - loss: 0.0934 - accuracy: 0.9720 - val_loss: 0.0660 - val_accuracy: 0.9808 - 6s/epoch - 14ms/step\n",
      "Epoch 61/200\n",
      "422/422 - 6s - loss: 0.0902 - accuracy: 0.9726 - val_loss: 0.0661 - val_accuracy: 0.9827 - 6s/epoch - 13ms/step\n",
      "Epoch 62/200\n",
      "422/422 - 5s - loss: 0.0893 - accuracy: 0.9723 - val_loss: 0.0659 - val_accuracy: 0.9812 - 5s/epoch - 13ms/step\n",
      "Epoch 63/200\n",
      "422/422 - 6s - loss: 0.0912 - accuracy: 0.9724 - val_loss: 0.0601 - val_accuracy: 0.9832 - 6s/epoch - 13ms/step\n",
      "Epoch 64/200\n",
      "422/422 - 6s - loss: 0.0883 - accuracy: 0.9726 - val_loss: 0.0594 - val_accuracy: 0.9832 - 6s/epoch - 13ms/step\n",
      "Epoch 65/200\n",
      "  >> Test@065 - loss: 0.0726 - acc: 0.9779\n",
      "422/422 - 6s - loss: 0.0865 - accuracy: 0.9735 - val_loss: 0.0604 - val_accuracy: 0.9828 - 6s/epoch - 14ms/step\n",
      "Epoch 66/200\n",
      "422/422 - 6s - loss: 0.0854 - accuracy: 0.9745 - val_loss: 0.0600 - val_accuracy: 0.9833 - 6s/epoch - 13ms/step\n",
      "Epoch 67/200\n",
      "422/422 - 6s - loss: 0.0863 - accuracy: 0.9738 - val_loss: 0.0603 - val_accuracy: 0.9825 - 6s/epoch - 13ms/step\n",
      "Epoch 68/200\n",
      "422/422 - 6s - loss: 0.0852 - accuracy: 0.9739 - val_loss: 0.0597 - val_accuracy: 0.9837 - 6s/epoch - 13ms/step\n",
      "Epoch 69/200\n",
      "422/422 - 5s - loss: 0.0833 - accuracy: 0.9740 - val_loss: 0.0621 - val_accuracy: 0.9820 - 5s/epoch - 13ms/step\n",
      "Epoch 70/200\n",
      "  >> Test@070 - loss: 0.0728 - acc: 0.9773\n",
      "422/422 - 6s - loss: 0.0865 - accuracy: 0.9730 - val_loss: 0.0599 - val_accuracy: 0.9827 - 6s/epoch - 13ms/step\n",
      "Epoch 71/200\n",
      "422/422 - 5s - loss: 0.0832 - accuracy: 0.9747 - val_loss: 0.0550 - val_accuracy: 0.9840 - 5s/epoch - 13ms/step\n",
      "Epoch 72/200\n",
      "422/422 - 5s - loss: 0.0831 - accuracy: 0.9749 - val_loss: 0.0579 - val_accuracy: 0.9827 - 5s/epoch - 13ms/step\n",
      "Epoch 73/200\n",
      "422/422 - 6s - loss: 0.0831 - accuracy: 0.9744 - val_loss: 0.0572 - val_accuracy: 0.9825 - 6s/epoch - 13ms/step\n",
      "Epoch 74/200\n",
      "422/422 - 6s - loss: 0.0823 - accuracy: 0.9755 - val_loss: 0.0582 - val_accuracy: 0.9842 - 6s/epoch - 13ms/step\n",
      "Epoch 75/200\n",
      "  >> Test@075 - loss: 0.0653 - acc: 0.9806\n",
      "422/422 - 6s - loss: 0.0814 - accuracy: 0.9752 - val_loss: 0.0558 - val_accuracy: 0.9843 - 6s/epoch - 14ms/step\n",
      "Epoch 76/200\n",
      "422/422 - 6s - loss: 0.0821 - accuracy: 0.9750 - val_loss: 0.0557 - val_accuracy: 0.9845 - 6s/epoch - 13ms/step\n",
      "Epoch 77/200\n",
      "422/422 - 6s - loss: 0.0808 - accuracy: 0.9753 - val_loss: 0.0539 - val_accuracy: 0.9857 - 6s/epoch - 13ms/step\n",
      "Epoch 78/200\n",
      "422/422 - 6s - loss: 0.0821 - accuracy: 0.9746 - val_loss: 0.0550 - val_accuracy: 0.9847 - 6s/epoch - 13ms/step\n",
      "Epoch 79/200\n",
      "422/422 - 6s - loss: 0.0795 - accuracy: 0.9759 - val_loss: 0.0544 - val_accuracy: 0.9840 - 6s/epoch - 14ms/step\n",
      "Epoch 80/200\n",
      "  >> Test@080 - loss: 0.0694 - acc: 0.9783\n",
      "422/422 - 6s - loss: 0.0783 - accuracy: 0.9757 - val_loss: 0.0545 - val_accuracy: 0.9833 - 6s/epoch - 14ms/step\n",
      "Epoch 81/200\n",
      "422/422 - 6s - loss: 0.0790 - accuracy: 0.9752 - val_loss: 0.0534 - val_accuracy: 0.9845 - 6s/epoch - 14ms/step\n",
      "Epoch 82/200\n",
      "422/422 - 6s - loss: 0.0785 - accuracy: 0.9758 - val_loss: 0.0561 - val_accuracy: 0.9833 - 6s/epoch - 14ms/step\n",
      "Epoch 83/200\n",
      "422/422 - 6s - loss: 0.0780 - accuracy: 0.9762 - val_loss: 0.0561 - val_accuracy: 0.9843 - 6s/epoch - 14ms/step\n",
      "Epoch 84/200\n",
      "422/422 - 6s - loss: 0.0795 - accuracy: 0.9753 - val_loss: 0.0589 - val_accuracy: 0.9837 - 6s/epoch - 14ms/step\n",
      "Epoch 85/200\n",
      "  >> Test@085 - loss: 0.0641 - acc: 0.9803\n",
      "422/422 - 6s - loss: 0.0778 - accuracy: 0.9763 - val_loss: 0.0526 - val_accuracy: 0.9845 - 6s/epoch - 14ms/step\n",
      "Epoch 86/200\n",
      "422/422 - 6s - loss: 0.0779 - accuracy: 0.9757 - val_loss: 0.0543 - val_accuracy: 0.9847 - 6s/epoch - 14ms/step\n",
      "Epoch 87/200\n",
      "422/422 - 6s - loss: 0.0772 - accuracy: 0.9755 - val_loss: 0.0566 - val_accuracy: 0.9840 - 6s/epoch - 14ms/step\n",
      "Epoch 88/200\n",
      "422/422 - 6s - loss: 0.0751 - accuracy: 0.9771 - val_loss: 0.0536 - val_accuracy: 0.9842 - 6s/epoch - 14ms/step\n",
      "Epoch 89/200\n",
      "422/422 - 6s - loss: 0.0769 - accuracy: 0.9766 - val_loss: 0.0544 - val_accuracy: 0.9838 - 6s/epoch - 14ms/step\n",
      "Epoch 90/200\n",
      "  >> Test@090 - loss: 0.0624 - acc: 0.9804\n",
      "422/422 - 6s - loss: 0.0762 - accuracy: 0.9765 - val_loss: 0.0527 - val_accuracy: 0.9852 - 6s/epoch - 14ms/step\n",
      "Epoch 91/200\n",
      "422/422 - 6s - loss: 0.0763 - accuracy: 0.9754 - val_loss: 0.0515 - val_accuracy: 0.9840 - 6s/epoch - 14ms/step\n",
      "Epoch 92/200\n",
      "422/422 - 6s - loss: 0.0749 - accuracy: 0.9769 - val_loss: 0.0529 - val_accuracy: 0.9838 - 6s/epoch - 13ms/step\n",
      "Epoch 93/200\n",
      "422/422 - 6s - loss: 0.0748 - accuracy: 0.9766 - val_loss: 0.0517 - val_accuracy: 0.9855 - 6s/epoch - 14ms/step\n",
      "Epoch 94/200\n",
      "422/422 - 6s - loss: 0.0737 - accuracy: 0.9767 - val_loss: 0.0520 - val_accuracy: 0.9838 - 6s/epoch - 13ms/step\n",
      "Epoch 95/200\n",
      "  >> Test@095 - loss: 0.0635 - acc: 0.9804\n",
      "422/422 - 6s - loss: 0.0748 - accuracy: 0.9767 - val_loss: 0.0566 - val_accuracy: 0.9843 - 6s/epoch - 14ms/step\n",
      "Epoch 96/200\n",
      "422/422 - 6s - loss: 0.0748 - accuracy: 0.9765 - val_loss: 0.0562 - val_accuracy: 0.9842 - 6s/epoch - 14ms/step\n",
      "Epoch 97/200\n",
      "422/422 - 6s - loss: 0.0759 - accuracy: 0.9761 - val_loss: 0.0524 - val_accuracy: 0.9852 - 6s/epoch - 13ms/step\n",
      "Epoch 98/200\n",
      "422/422 - 6s - loss: 0.0740 - accuracy: 0.9773 - val_loss: 0.0543 - val_accuracy: 0.9852 - 6s/epoch - 14ms/step\n",
      "Epoch 99/200\n",
      "422/422 - 6s - loss: 0.0748 - accuracy: 0.9767 - val_loss: 0.0562 - val_accuracy: 0.9843 - 6s/epoch - 14ms/step\n",
      "Epoch 100/200\n",
      "  >> Test@100 - loss: 0.0662 - acc: 0.9799\n",
      "422/422 - 6s - loss: 0.0726 - accuracy: 0.9767 - val_loss: 0.0557 - val_accuracy: 0.9838 - 6s/epoch - 14ms/step\n",
      "Epoch 101/200\n",
      "422/422 - 6s - loss: 0.0735 - accuracy: 0.9771 - val_loss: 0.0561 - val_accuracy: 0.9842 - 6s/epoch - 14ms/step\n",
      "Epoch 102/200\n",
      "422/422 - 6s - loss: 0.0708 - accuracy: 0.9783 - val_loss: 0.0552 - val_accuracy: 0.9830 - 6s/epoch - 14ms/step\n",
      "Epoch 103/200\n",
      "422/422 - 6s - loss: 0.0727 - accuracy: 0.9776 - val_loss: 0.0520 - val_accuracy: 0.9850 - 6s/epoch - 14ms/step\n",
      "Epoch 104/200\n",
      "422/422 - 6s - loss: 0.0717 - accuracy: 0.9779 - val_loss: 0.0542 - val_accuracy: 0.9840 - 6s/epoch - 14ms/step\n",
      "Epoch 105/200\n",
      "  >> Test@105 - loss: 0.0622 - acc: 0.9803\n",
      "422/422 - 6s - loss: 0.0710 - accuracy: 0.9780 - val_loss: 0.0506 - val_accuracy: 0.9843 - 6s/epoch - 14ms/step\n",
      "Epoch 106/200\n",
      "422/422 - 6s - loss: 0.0718 - accuracy: 0.9778 - val_loss: 0.0514 - val_accuracy: 0.9848 - 6s/epoch - 14ms/step\n",
      "Epoch 107/200\n",
      "422/422 - 6s - loss: 0.0712 - accuracy: 0.9781 - val_loss: 0.0521 - val_accuracy: 0.9845 - 6s/epoch - 14ms/step\n",
      "Epoch 108/200\n",
      "422/422 - 6s - loss: 0.0721 - accuracy: 0.9773 - val_loss: 0.0539 - val_accuracy: 0.9840 - 6s/epoch - 14ms/step\n",
      "Epoch 109/200\n",
      "422/422 - 5s - loss: 0.0712 - accuracy: 0.9782 - val_loss: 0.0551 - val_accuracy: 0.9852 - 5s/epoch - 13ms/step\n",
      "Epoch 110/200\n",
      "  >> Test@110 - loss: 0.0574 - acc: 0.9826\n",
      "422/422 - 6s - loss: 0.0712 - accuracy: 0.9777 - val_loss: 0.0493 - val_accuracy: 0.9850 - 6s/epoch - 13ms/step\n",
      "Epoch 111/200\n",
      "422/422 - 5s - loss: 0.0711 - accuracy: 0.9782 - val_loss: 0.0542 - val_accuracy: 0.9850 - 5s/epoch - 13ms/step\n",
      "Epoch 112/200\n",
      "422/422 - 5s - loss: 0.0722 - accuracy: 0.9776 - val_loss: 0.0504 - val_accuracy: 0.9857 - 5s/epoch - 13ms/step\n",
      "Epoch 113/200\n",
      "422/422 - 5s - loss: 0.0713 - accuracy: 0.9779 - val_loss: 0.0548 - val_accuracy: 0.9832 - 5s/epoch - 13ms/step\n",
      "Epoch 114/200\n",
      "422/422 - 5s - loss: 0.0699 - accuracy: 0.9781 - val_loss: 0.0509 - val_accuracy: 0.9847 - 5s/epoch - 13ms/step\n",
      "Epoch 115/200\n",
      "  >> Test@115 - loss: 0.0609 - acc: 0.9809\n",
      "422/422 - 6s - loss: 0.0701 - accuracy: 0.9776 - val_loss: 0.0582 - val_accuracy: 0.9813 - 6s/epoch - 14ms/step\n",
      "Epoch 116/200\n",
      "422/422 - 5s - loss: 0.0693 - accuracy: 0.9786 - val_loss: 0.0515 - val_accuracy: 0.9840 - 5s/epoch - 13ms/step\n",
      "Epoch 117/200\n",
      "422/422 - 5s - loss: 0.0693 - accuracy: 0.9776 - val_loss: 0.0510 - val_accuracy: 0.9862 - 5s/epoch - 13ms/step\n",
      "Epoch 118/200\n",
      "422/422 - 5s - loss: 0.0683 - accuracy: 0.9789 - val_loss: 0.0497 - val_accuracy: 0.9852 - 5s/epoch - 13ms/step\n",
      "Epoch 119/200\n",
      "422/422 - 5s - loss: 0.0673 - accuracy: 0.9793 - val_loss: 0.0526 - val_accuracy: 0.9845 - 5s/epoch - 13ms/step\n",
      "Epoch 120/200\n",
      "  >> Test@120 - loss: 0.0646 - acc: 0.9798\n",
      "422/422 - 6s - loss: 0.0658 - accuracy: 0.9796 - val_loss: 0.0560 - val_accuracy: 0.9827 - 6s/epoch - 14ms/step\n",
      "Epoch 121/200\n",
      "422/422 - 5s - loss: 0.0686 - accuracy: 0.9780 - val_loss: 0.0522 - val_accuracy: 0.9850 - 5s/epoch - 13ms/step\n",
      "Epoch 122/200\n",
      "422/422 - 5s - loss: 0.0689 - accuracy: 0.9781 - val_loss: 0.0483 - val_accuracy: 0.9848 - 5s/epoch - 13ms/step\n",
      "Epoch 123/200\n",
      "422/422 - 5s - loss: 0.0689 - accuracy: 0.9787 - val_loss: 0.0500 - val_accuracy: 0.9858 - 5s/epoch - 13ms/step\n",
      "Epoch 124/200\n",
      "422/422 - 6s - loss: 0.0669 - accuracy: 0.9794 - val_loss: 0.0502 - val_accuracy: 0.9845 - 6s/epoch - 13ms/step\n",
      "Epoch 125/200\n",
      "  >> Test@125 - loss: 0.0573 - acc: 0.9825\n",
      "422/422 - 6s - loss: 0.0658 - accuracy: 0.9797 - val_loss: 0.0476 - val_accuracy: 0.9845 - 6s/epoch - 14ms/step\n",
      "Epoch 126/200\n",
      "422/422 - 6s - loss: 0.0685 - accuracy: 0.9786 - val_loss: 0.0489 - val_accuracy: 0.9860 - 6s/epoch - 13ms/step\n",
      "Epoch 127/200\n",
      "422/422 - 5s - loss: 0.0675 - accuracy: 0.9790 - val_loss: 0.0481 - val_accuracy: 0.9860 - 5s/epoch - 13ms/step\n",
      "Epoch 128/200\n",
      "422/422 - 5s - loss: 0.0682 - accuracy: 0.9793 - val_loss: 0.0513 - val_accuracy: 0.9850 - 5s/epoch - 12ms/step\n",
      "Epoch 129/200\n",
      "422/422 - 5s - loss: 0.0651 - accuracy: 0.9798 - val_loss: 0.0465 - val_accuracy: 0.9860 - 5s/epoch - 13ms/step\n",
      "Epoch 130/200\n",
      "  >> Test@130 - loss: 0.0594 - acc: 0.9812\n",
      "422/422 - 6s - loss: 0.0670 - accuracy: 0.9791 - val_loss: 0.0495 - val_accuracy: 0.9855 - 6s/epoch - 14ms/step\n",
      "Epoch 131/200\n",
      "422/422 - 5s - loss: 0.0674 - accuracy: 0.9787 - val_loss: 0.0495 - val_accuracy: 0.9855 - 5s/epoch - 13ms/step\n",
      "Epoch 132/200\n",
      "422/422 - 6s - loss: 0.0673 - accuracy: 0.9791 - val_loss: 0.0540 - val_accuracy: 0.9847 - 6s/epoch - 13ms/step\n",
      "Epoch 133/200\n",
      "422/422 - 5s - loss: 0.0678 - accuracy: 0.9786 - val_loss: 0.0508 - val_accuracy: 0.9853 - 5s/epoch - 13ms/step\n",
      "Epoch 134/200\n",
      "422/422 - 6s - loss: 0.0668 - accuracy: 0.9789 - val_loss: 0.0510 - val_accuracy: 0.9860 - 6s/epoch - 13ms/step\n",
      "Epoch 135/200\n",
      "  >> Test@135 - loss: 0.0581 - acc: 0.9829\n",
      "422/422 - 6s - loss: 0.0664 - accuracy: 0.9791 - val_loss: 0.0540 - val_accuracy: 0.9843 - 6s/epoch - 14ms/step\n",
      "Epoch 136/200\n",
      "422/422 - 6s - loss: 0.0627 - accuracy: 0.9808 - val_loss: 0.0518 - val_accuracy: 0.9857 - 6s/epoch - 13ms/step\n",
      "Epoch 137/200\n",
      "422/422 - 6s - loss: 0.0652 - accuracy: 0.9794 - val_loss: 0.0475 - val_accuracy: 0.9863 - 6s/epoch - 13ms/step\n",
      "Epoch 138/200\n",
      "422/422 - 5s - loss: 0.0649 - accuracy: 0.9790 - val_loss: 0.0480 - val_accuracy: 0.9850 - 5s/epoch - 13ms/step\n",
      "Epoch 139/200\n",
      "422/422 - 6s - loss: 0.0653 - accuracy: 0.9798 - val_loss: 0.0486 - val_accuracy: 0.9852 - 6s/epoch - 13ms/step\n",
      "Epoch 140/200\n",
      "  >> Test@140 - loss: 0.0570 - acc: 0.9812\n",
      "422/422 - 6s - loss: 0.0638 - accuracy: 0.9803 - val_loss: 0.0491 - val_accuracy: 0.9845 - 6s/epoch - 14ms/step\n",
      "Epoch 141/200\n",
      "422/422 - 6s - loss: 0.0651 - accuracy: 0.9793 - val_loss: 0.0499 - val_accuracy: 0.9845 - 6s/epoch - 13ms/step\n",
      "Epoch 142/200\n",
      "422/422 - 5s - loss: 0.0664 - accuracy: 0.9787 - val_loss: 0.0499 - val_accuracy: 0.9855 - 5s/epoch - 13ms/step\n",
      "Epoch 143/200\n",
      "422/422 - 5s - loss: 0.0650 - accuracy: 0.9792 - val_loss: 0.0532 - val_accuracy: 0.9858 - 5s/epoch - 13ms/step\n",
      "Epoch 144/200\n",
      "422/422 - 5s - loss: 0.0634 - accuracy: 0.9809 - val_loss: 0.0489 - val_accuracy: 0.9862 - 5s/epoch - 13ms/step\n",
      "Epoch 145/200\n",
      "  >> Test@145 - loss: 0.0558 - acc: 0.9830\n",
      "422/422 - 6s - loss: 0.0639 - accuracy: 0.9795 - val_loss: 0.0497 - val_accuracy: 0.9862 - 6s/epoch - 14ms/step\n",
      "Epoch 146/200\n",
      "422/422 - 6s - loss: 0.0669 - accuracy: 0.9792 - val_loss: 0.0465 - val_accuracy: 0.9867 - 6s/epoch - 13ms/step\n",
      "Epoch 147/200\n",
      "422/422 - 6s - loss: 0.0632 - accuracy: 0.9802 - val_loss: 0.0495 - val_accuracy: 0.9855 - 6s/epoch - 13ms/step\n",
      "Epoch 148/200\n",
      "422/422 - 5s - loss: 0.0629 - accuracy: 0.9800 - val_loss: 0.0557 - val_accuracy: 0.9842 - 5s/epoch - 13ms/step\n",
      "Epoch 149/200\n",
      "422/422 - 5s - loss: 0.0637 - accuracy: 0.9798 - val_loss: 0.0482 - val_accuracy: 0.9857 - 5s/epoch - 13ms/step\n",
      "Epoch 150/200\n",
      "  >> Test@150 - loss: 0.0542 - acc: 0.9831\n",
      "422/422 - 6s - loss: 0.0630 - accuracy: 0.9804 - val_loss: 0.0482 - val_accuracy: 0.9858 - 6s/epoch - 14ms/step\n",
      "Epoch 151/200\n",
      "422/422 - 5s - loss: 0.0637 - accuracy: 0.9798 - val_loss: 0.0477 - val_accuracy: 0.9863 - 5s/epoch - 13ms/step\n",
      "Epoch 152/200\n",
      "422/422 - 5s - loss: 0.0616 - accuracy: 0.9808 - val_loss: 0.0471 - val_accuracy: 0.9855 - 5s/epoch - 13ms/step\n",
      "Epoch 153/200\n",
      "422/422 - 5s - loss: 0.0626 - accuracy: 0.9805 - val_loss: 0.0469 - val_accuracy: 0.9863 - 5s/epoch - 13ms/step\n",
      "Epoch 154/200\n",
      "422/422 - 5s - loss: 0.0624 - accuracy: 0.9800 - val_loss: 0.0476 - val_accuracy: 0.9870 - 5s/epoch - 13ms/step\n",
      "Epoch 155/200\n",
      "  >> Test@155 - loss: 0.0561 - acc: 0.9828\n",
      "422/422 - 6s - loss: 0.0637 - accuracy: 0.9804 - val_loss: 0.0465 - val_accuracy: 0.9870 - 6s/epoch - 14ms/step\n",
      "Epoch 156/200\n",
      "422/422 - 5s - loss: 0.0619 - accuracy: 0.9806 - val_loss: 0.0528 - val_accuracy: 0.9855 - 5s/epoch - 13ms/step\n",
      "Epoch 157/200\n",
      "422/422 - 6s - loss: 0.0623 - accuracy: 0.9810 - val_loss: 0.0467 - val_accuracy: 0.9858 - 6s/epoch - 13ms/step\n",
      "Epoch 158/200\n",
      "422/422 - 5s - loss: 0.0623 - accuracy: 0.9803 - val_loss: 0.0471 - val_accuracy: 0.9870 - 5s/epoch - 13ms/step\n",
      "Epoch 159/200\n",
      "422/422 - 5s - loss: 0.0621 - accuracy: 0.9804 - val_loss: 0.0465 - val_accuracy: 0.9862 - 5s/epoch - 13ms/step\n",
      "Epoch 160/200\n",
      "  >> Test@160 - loss: 0.0519 - acc: 0.9824\n",
      "422/422 - 6s - loss: 0.0613 - accuracy: 0.9811 - val_loss: 0.0476 - val_accuracy: 0.9873 - 6s/epoch - 14ms/step\n",
      "Epoch 161/200\n",
      "422/422 - 5s - loss: 0.0634 - accuracy: 0.9799 - val_loss: 0.0475 - val_accuracy: 0.9875 - 5s/epoch - 13ms/step\n",
      "Epoch 162/200\n",
      "422/422 - 5s - loss: 0.0634 - accuracy: 0.9804 - val_loss: 0.0460 - val_accuracy: 0.9867 - 5s/epoch - 13ms/step\n",
      "Epoch 163/200\n",
      "422/422 - 6s - loss: 0.0630 - accuracy: 0.9800 - val_loss: 0.0477 - val_accuracy: 0.9853 - 6s/epoch - 13ms/step\n",
      "Epoch 164/200\n",
      "422/422 - 5s - loss: 0.0642 - accuracy: 0.9798 - val_loss: 0.0477 - val_accuracy: 0.9870 - 5s/epoch - 13ms/step\n",
      "Epoch 165/200\n",
      "  >> Test@165 - loss: 0.0527 - acc: 0.9847\n",
      "422/422 - 6s - loss: 0.0633 - accuracy: 0.9799 - val_loss: 0.0471 - val_accuracy: 0.9860 - 6s/epoch - 13ms/step\n",
      "Epoch 166/200\n",
      "422/422 - 6s - loss: 0.0600 - accuracy: 0.9810 - val_loss: 0.0463 - val_accuracy: 0.9862 - 6s/epoch - 13ms/step\n",
      "Epoch 167/200\n",
      "422/422 - 5s - loss: 0.0619 - accuracy: 0.9808 - val_loss: 0.0482 - val_accuracy: 0.9853 - 5s/epoch - 13ms/step\n",
      "Epoch 168/200\n",
      "422/422 - 5s - loss: 0.0612 - accuracy: 0.9808 - val_loss: 0.0507 - val_accuracy: 0.9840 - 5s/epoch - 13ms/step\n",
      "Epoch 169/200\n",
      "422/422 - 7s - loss: 0.0606 - accuracy: 0.9807 - val_loss: 0.0476 - val_accuracy: 0.9852 - 7s/epoch - 16ms/step\n",
      "Epoch 170/200\n",
      "  >> Test@170 - loss: 0.0588 - acc: 0.9824\n",
      "422/422 - 7s - loss: 0.0647 - accuracy: 0.9795 - val_loss: 0.0506 - val_accuracy: 0.9853 - 7s/epoch - 17ms/step\n",
      "Epoch 171/200\n",
      "422/422 - 7s - loss: 0.0616 - accuracy: 0.9806 - val_loss: 0.0469 - val_accuracy: 0.9865 - 7s/epoch - 16ms/step\n",
      "Epoch 172/200\n",
      "422/422 - 7s - loss: 0.0616 - accuracy: 0.9801 - val_loss: 0.0472 - val_accuracy: 0.9850 - 7s/epoch - 16ms/step\n",
      "Epoch 173/200\n",
      "422/422 - 7s - loss: 0.0615 - accuracy: 0.9805 - val_loss: 0.0462 - val_accuracy: 0.9872 - 7s/epoch - 17ms/step\n",
      "Epoch 174/200\n",
      "422/422 - 7s - loss: 0.0611 - accuracy: 0.9810 - val_loss: 0.0515 - val_accuracy: 0.9862 - 7s/epoch - 17ms/step\n",
      "Epoch 175/200\n",
      "  >> Test@175 - loss: 0.0537 - acc: 0.9842\n",
      "422/422 - 7s - loss: 0.0632 - accuracy: 0.9799 - val_loss: 0.0467 - val_accuracy: 0.9862 - 7s/epoch - 17ms/step\n",
      "Epoch 176/200\n",
      "422/422 - 7s - loss: 0.0611 - accuracy: 0.9801 - val_loss: 0.0485 - val_accuracy: 0.9858 - 7s/epoch - 16ms/step\n",
      "Epoch 177/200\n",
      "422/422 - 7s - loss: 0.0606 - accuracy: 0.9804 - val_loss: 0.0473 - val_accuracy: 0.9862 - 7s/epoch - 17ms/step\n",
      "Epoch 178/200\n",
      "422/422 - 7s - loss: 0.0610 - accuracy: 0.9806 - val_loss: 0.0474 - val_accuracy: 0.9855 - 7s/epoch - 16ms/step\n",
      "Epoch 179/200\n",
      "422/422 - 7s - loss: 0.0631 - accuracy: 0.9799 - val_loss: 0.0488 - val_accuracy: 0.9848 - 7s/epoch - 16ms/step\n",
      "Epoch 180/200\n",
      "  >> Test@180 - loss: 0.0542 - acc: 0.9829\n",
      "422/422 - 7s - loss: 0.0615 - accuracy: 0.9808 - val_loss: 0.0494 - val_accuracy: 0.9843 - 7s/epoch - 17ms/step\n",
      "Epoch 181/200\n",
      "422/422 - 7s - loss: 0.0586 - accuracy: 0.9811 - val_loss: 0.0507 - val_accuracy: 0.9848 - 7s/epoch - 15ms/step\n",
      "Epoch 182/200\n",
      "422/422 - 7s - loss: 0.0602 - accuracy: 0.9810 - val_loss: 0.0469 - val_accuracy: 0.9863 - 7s/epoch - 16ms/step\n",
      "Epoch 183/200\n",
      "422/422 - 7s - loss: 0.0614 - accuracy: 0.9794 - val_loss: 0.0471 - val_accuracy: 0.9853 - 7s/epoch - 16ms/step\n",
      "Epoch 184/200\n",
      "422/422 - 7s - loss: 0.0627 - accuracy: 0.9800 - val_loss: 0.0467 - val_accuracy: 0.9855 - 7s/epoch - 16ms/step\n",
      "Epoch 185/200\n",
      "  >> Test@185 - loss: 0.0579 - acc: 0.9824\n",
      "422/422 - 7s - loss: 0.0611 - accuracy: 0.9808 - val_loss: 0.0473 - val_accuracy: 0.9863 - 7s/epoch - 17ms/step\n",
      "Epoch 186/200\n",
      "422/422 - 7s - loss: 0.0600 - accuracy: 0.9804 - val_loss: 0.0462 - val_accuracy: 0.9863 - 7s/epoch - 17ms/step\n",
      "Epoch 187/200\n",
      "422/422 - 7s - loss: 0.0604 - accuracy: 0.9807 - val_loss: 0.0449 - val_accuracy: 0.9870 - 7s/epoch - 16ms/step\n",
      "Epoch 188/200\n",
      "  >> Reached target test acc 0.9850 at epoch 188. Stopping.\n",
      "422/422 - 7s - loss: 0.0601 - accuracy: 0.9807 - val_loss: 0.0443 - val_accuracy: 0.9863 - 7s/epoch - 17ms/step\n",
      "Done: SET (RAND) | Final TestAcc=0.9850 | nW=128564\n",
      "\n",
      "\n",
      " Training: SET (IMP, Î±=0.75)\n",
      "Epoch 1/200\n",
      "422/422 - 9s - loss: 2.1995 - accuracy: 0.3382 - val_loss: 1.9584 - val_accuracy: 0.6472 - 9s/epoch - 22ms/step\n",
      "Epoch 2/200\n",
      "422/422 - 7s - loss: 1.2089 - accuracy: 0.6980 - val_loss: 0.5963 - val_accuracy: 0.8490 - 7s/epoch - 15ms/step\n",
      "Epoch 3/200\n",
      "422/422 - 7s - loss: 0.5713 - accuracy: 0.8317 - val_loss: 0.3790 - val_accuracy: 0.8980 - 7s/epoch - 16ms/step\n",
      "Epoch 4/200\n",
      "422/422 - 7s - loss: 0.4540 - accuracy: 0.8663 - val_loss: 0.3221 - val_accuracy: 0.9080 - 7s/epoch - 16ms/step\n",
      "Epoch 5/200\n",
      "  >> Test@005 - loss: 0.3432 - acc: 0.9015\n",
      "422/422 - 7s - loss: 0.4091 - accuracy: 0.8810 - val_loss: 0.2940 - val_accuracy: 0.9207 - 7s/epoch - 17ms/step\n",
      "Epoch 6/200\n",
      "422/422 - 7s - loss: 0.3872 - accuracy: 0.8880 - val_loss: 0.2812 - val_accuracy: 0.9197 - 7s/epoch - 16ms/step\n",
      "Epoch 7/200\n",
      "422/422 - 7s - loss: 0.3719 - accuracy: 0.8921 - val_loss: 0.2676 - val_accuracy: 0.9240 - 7s/epoch - 16ms/step\n",
      "Epoch 8/200\n",
      "422/422 - 7s - loss: 0.3591 - accuracy: 0.8963 - val_loss: 0.2608 - val_accuracy: 0.9250 - 7s/epoch - 16ms/step\n",
      "Epoch 9/200\n",
      "422/422 - 7s - loss: 0.3506 - accuracy: 0.8988 - val_loss: 0.2573 - val_accuracy: 0.9275 - 7s/epoch - 16ms/step\n",
      "Epoch 10/200\n",
      "  >> Test@010 - loss: 0.3029 - acc: 0.9105\n",
      "422/422 - 8s - loss: 0.3416 - accuracy: 0.9009 - val_loss: 0.2508 - val_accuracy: 0.9298 - 8s/epoch - 18ms/step\n",
      "Epoch 11/200\n",
      "422/422 - 7s - loss: 0.3319 - accuracy: 0.9028 - val_loss: 0.2396 - val_accuracy: 0.9315 - 7s/epoch - 16ms/step\n",
      "Epoch 12/200\n",
      "422/422 - 7s - loss: 0.3138 - accuracy: 0.9077 - val_loss: 0.2233 - val_accuracy: 0.9345 - 7s/epoch - 17ms/step\n",
      "Epoch 13/200\n",
      "422/422 - 7s - loss: 0.2968 - accuracy: 0.9132 - val_loss: 0.2069 - val_accuracy: 0.9397 - 7s/epoch - 17ms/step\n",
      "Epoch 14/200\n",
      "422/422 - 7s - loss: 0.2756 - accuracy: 0.9192 - val_loss: 0.1848 - val_accuracy: 0.9483 - 7s/epoch - 17ms/step\n",
      "Epoch 15/200\n",
      "  >> Test@015 - loss: 0.2195 - acc: 0.9329\n",
      "422/422 - 7s - loss: 0.2579 - accuracy: 0.9228 - val_loss: 0.1700 - val_accuracy: 0.9535 - 7s/epoch - 17ms/step\n",
      "Epoch 16/200\n",
      "422/422 - 7s - loss: 0.2396 - accuracy: 0.9284 - val_loss: 0.1557 - val_accuracy: 0.9582 - 7s/epoch - 17ms/step\n",
      "Epoch 17/200\n",
      "422/422 - 7s - loss: 0.2276 - accuracy: 0.9326 - val_loss: 0.1393 - val_accuracy: 0.9632 - 7s/epoch - 17ms/step\n",
      "Epoch 18/200\n",
      "422/422 - 7s - loss: 0.2087 - accuracy: 0.9384 - val_loss: 0.1331 - val_accuracy: 0.9645 - 7s/epoch - 17ms/step\n",
      "Epoch 19/200\n",
      "422/422 - 7s - loss: 0.1979 - accuracy: 0.9409 - val_loss: 0.1238 - val_accuracy: 0.9662 - 7s/epoch - 17ms/step\n",
      "Epoch 20/200\n",
      "  >> Test@020 - loss: 0.1439 - acc: 0.9556\n",
      "422/422 - 7s - loss: 0.1870 - accuracy: 0.9434 - val_loss: 0.1151 - val_accuracy: 0.9705 - 7s/epoch - 18ms/step\n",
      "Epoch 21/200\n",
      "422/422 - 7s - loss: 0.1810 - accuracy: 0.9449 - val_loss: 0.1093 - val_accuracy: 0.9707 - 7s/epoch - 17ms/step\n",
      "Epoch 22/200\n",
      "422/422 - 7s - loss: 0.1728 - accuracy: 0.9469 - val_loss: 0.1055 - val_accuracy: 0.9717 - 7s/epoch - 17ms/step\n",
      "Epoch 23/200\n",
      "422/422 - 7s - loss: 0.1655 - accuracy: 0.9501 - val_loss: 0.0996 - val_accuracy: 0.9747 - 7s/epoch - 17ms/step\n",
      "Epoch 24/200\n",
      "422/422 - 7s - loss: 0.1624 - accuracy: 0.9510 - val_loss: 0.1016 - val_accuracy: 0.9713 - 7s/epoch - 16ms/step\n",
      "Epoch 25/200\n",
      "  >> Test@025 - loss: 0.1231 - acc: 0.9619\n",
      "422/422 - 8s - loss: 0.1549 - accuracy: 0.9528 - val_loss: 0.0990 - val_accuracy: 0.9728 - 8s/epoch - 18ms/step\n",
      "Epoch 26/200\n",
      "422/422 - 7s - loss: 0.1489 - accuracy: 0.9543 - val_loss: 0.0937 - val_accuracy: 0.9750 - 7s/epoch - 17ms/step\n",
      "Epoch 27/200\n",
      "422/422 - 7s - loss: 0.1467 - accuracy: 0.9547 - val_loss: 0.0913 - val_accuracy: 0.9753 - 7s/epoch - 16ms/step\n",
      "Epoch 28/200\n",
      "422/422 - 7s - loss: 0.1392 - accuracy: 0.9568 - val_loss: 0.0900 - val_accuracy: 0.9762 - 7s/epoch - 17ms/step\n",
      "Epoch 29/200\n",
      "422/422 - 7s - loss: 0.1376 - accuracy: 0.9577 - val_loss: 0.0920 - val_accuracy: 0.9763 - 7s/epoch - 17ms/step\n",
      "Epoch 30/200\n",
      "  >> Test@030 - loss: 0.1071 - acc: 0.9674\n",
      "422/422 - 7s - loss: 0.1330 - accuracy: 0.9589 - val_loss: 0.0859 - val_accuracy: 0.9758 - 7s/epoch - 17ms/step\n",
      "Epoch 31/200\n",
      "422/422 - 7s - loss: 0.1309 - accuracy: 0.9598 - val_loss: 0.0871 - val_accuracy: 0.9777 - 7s/epoch - 17ms/step\n",
      "Epoch 32/200\n",
      "422/422 - 7s - loss: 0.1253 - accuracy: 0.9620 - val_loss: 0.0838 - val_accuracy: 0.9777 - 7s/epoch - 17ms/step\n",
      "Epoch 33/200\n",
      "422/422 - 7s - loss: 0.1243 - accuracy: 0.9622 - val_loss: 0.0854 - val_accuracy: 0.9755 - 7s/epoch - 17ms/step\n",
      "Epoch 34/200\n",
      "422/422 - 7s - loss: 0.1205 - accuracy: 0.9625 - val_loss: 0.0820 - val_accuracy: 0.9777 - 7s/epoch - 17ms/step\n",
      "Epoch 35/200\n",
      "  >> Test@035 - loss: 0.0922 - acc: 0.9709\n",
      "422/422 - 8s - loss: 0.1206 - accuracy: 0.9633 - val_loss: 0.0794 - val_accuracy: 0.9795 - 8s/epoch - 18ms/step\n",
      "Epoch 36/200\n",
      "422/422 - 7s - loss: 0.1176 - accuracy: 0.9628 - val_loss: 0.0784 - val_accuracy: 0.9790 - 7s/epoch - 16ms/step\n",
      "Epoch 37/200\n",
      "422/422 - 7s - loss: 0.1151 - accuracy: 0.9643 - val_loss: 0.0786 - val_accuracy: 0.9773 - 7s/epoch - 17ms/step\n",
      "Epoch 38/200\n",
      "422/422 - 7s - loss: 0.1130 - accuracy: 0.9652 - val_loss: 0.0757 - val_accuracy: 0.9802 - 7s/epoch - 17ms/step\n",
      "Epoch 39/200\n",
      "422/422 - 7s - loss: 0.1122 - accuracy: 0.9652 - val_loss: 0.0737 - val_accuracy: 0.9805 - 7s/epoch - 16ms/step\n",
      "Epoch 40/200\n",
      "  >> Test@040 - loss: 0.0867 - acc: 0.9729\n",
      "422/422 - 8s - loss: 0.1092 - accuracy: 0.9670 - val_loss: 0.0713 - val_accuracy: 0.9808 - 8s/epoch - 18ms/step\n",
      "Epoch 41/200\n",
      "422/422 - 7s - loss: 0.1060 - accuracy: 0.9671 - val_loss: 0.0728 - val_accuracy: 0.9797 - 7s/epoch - 17ms/step\n",
      "Epoch 42/200\n",
      "422/422 - 7s - loss: 0.1045 - accuracy: 0.9670 - val_loss: 0.0735 - val_accuracy: 0.9795 - 7s/epoch - 17ms/step\n",
      "Epoch 43/200\n",
      "422/422 - 7s - loss: 0.1055 - accuracy: 0.9678 - val_loss: 0.0713 - val_accuracy: 0.9817 - 7s/epoch - 17ms/step\n",
      "Epoch 44/200\n",
      "422/422 - 7s - loss: 0.1014 - accuracy: 0.9692 - val_loss: 0.0692 - val_accuracy: 0.9810 - 7s/epoch - 16ms/step\n",
      "Epoch 45/200\n",
      "  >> Test@045 - loss: 0.0851 - acc: 0.9741\n",
      "422/422 - 7s - loss: 0.1013 - accuracy: 0.9690 - val_loss: 0.0669 - val_accuracy: 0.9805 - 7s/epoch - 17ms/step\n",
      "Epoch 46/200\n",
      "422/422 - 7s - loss: 0.0991 - accuracy: 0.9684 - val_loss: 0.0664 - val_accuracy: 0.9833 - 7s/epoch - 17ms/step\n",
      "Epoch 47/200\n",
      "422/422 - 7s - loss: 0.0986 - accuracy: 0.9689 - val_loss: 0.0727 - val_accuracy: 0.9805 - 7s/epoch - 17ms/step\n",
      "Epoch 48/200\n",
      "422/422 - 7s - loss: 0.0962 - accuracy: 0.9695 - val_loss: 0.0691 - val_accuracy: 0.9813 - 7s/epoch - 16ms/step\n",
      "Epoch 49/200\n",
      "422/422 - 7s - loss: 0.0955 - accuracy: 0.9701 - val_loss: 0.0675 - val_accuracy: 0.9817 - 7s/epoch - 17ms/step\n",
      "Epoch 50/200\n",
      "  >> Test@050 - loss: 0.0822 - acc: 0.9722\n",
      "422/422 - 7s - loss: 0.0952 - accuracy: 0.9702 - val_loss: 0.0666 - val_accuracy: 0.9817 - 7s/epoch - 18ms/step\n",
      "Epoch 51/200\n",
      "422/422 - 7s - loss: 0.0924 - accuracy: 0.9710 - val_loss: 0.0682 - val_accuracy: 0.9823 - 7s/epoch - 16ms/step\n",
      "Epoch 52/200\n",
      "422/422 - 7s - loss: 0.0925 - accuracy: 0.9720 - val_loss: 0.0631 - val_accuracy: 0.9818 - 7s/epoch - 17ms/step\n",
      "Epoch 53/200\n",
      "422/422 - 7s - loss: 0.0908 - accuracy: 0.9719 - val_loss: 0.0652 - val_accuracy: 0.9828 - 7s/epoch - 17ms/step\n",
      "Epoch 54/200\n",
      "422/422 - 7s - loss: 0.0910 - accuracy: 0.9717 - val_loss: 0.0632 - val_accuracy: 0.9823 - 7s/epoch - 16ms/step\n",
      "Epoch 55/200\n",
      "  >> Test@055 - loss: 0.0759 - acc: 0.9760\n",
      "422/422 - 7s - loss: 0.0906 - accuracy: 0.9722 - val_loss: 0.0641 - val_accuracy: 0.9840 - 7s/epoch - 18ms/step\n",
      "Epoch 56/200\n",
      "422/422 - 7s - loss: 0.0849 - accuracy: 0.9734 - val_loss: 0.0659 - val_accuracy: 0.9822 - 7s/epoch - 17ms/step\n",
      "Epoch 57/200\n",
      "422/422 - 7s - loss: 0.0856 - accuracy: 0.9735 - val_loss: 0.0611 - val_accuracy: 0.9825 - 7s/epoch - 16ms/step\n",
      "Epoch 58/200\n",
      "422/422 - 7s - loss: 0.0866 - accuracy: 0.9725 - val_loss: 0.0602 - val_accuracy: 0.9830 - 7s/epoch - 17ms/step\n",
      "Epoch 59/200\n",
      "422/422 - 7s - loss: 0.0866 - accuracy: 0.9723 - val_loss: 0.0614 - val_accuracy: 0.9822 - 7s/epoch - 17ms/step\n",
      "Epoch 60/200\n",
      "  >> Test@060 - loss: 0.0758 - acc: 0.9761\n",
      "422/422 - 7s - loss: 0.0865 - accuracy: 0.9732 - val_loss: 0.0614 - val_accuracy: 0.9830 - 7s/epoch - 17ms/step\n",
      "Epoch 61/200\n",
      "422/422 - 7s - loss: 0.0850 - accuracy: 0.9732 - val_loss: 0.0599 - val_accuracy: 0.9835 - 7s/epoch - 17ms/step\n",
      "Epoch 62/200\n",
      "422/422 - 7s - loss: 0.0859 - accuracy: 0.9733 - val_loss: 0.0607 - val_accuracy: 0.9830 - 7s/epoch - 17ms/step\n",
      "Epoch 63/200\n",
      "422/422 - 7s - loss: 0.0836 - accuracy: 0.9721 - val_loss: 0.0589 - val_accuracy: 0.9822 - 7s/epoch - 17ms/step\n",
      "Epoch 64/200\n",
      "422/422 - 7s - loss: 0.0824 - accuracy: 0.9746 - val_loss: 0.0587 - val_accuracy: 0.9843 - 7s/epoch - 17ms/step\n",
      "Epoch 65/200\n",
      "  >> Test@065 - loss: 0.0685 - acc: 0.9788\n",
      "422/422 - 7s - loss: 0.0817 - accuracy: 0.9739 - val_loss: 0.0593 - val_accuracy: 0.9832 - 7s/epoch - 18ms/step\n",
      "Epoch 66/200\n",
      "422/422 - 7s - loss: 0.0825 - accuracy: 0.9742 - val_loss: 0.0587 - val_accuracy: 0.9850 - 7s/epoch - 17ms/step\n",
      "Epoch 67/200\n",
      "422/422 - 7s - loss: 0.0830 - accuracy: 0.9737 - val_loss: 0.0590 - val_accuracy: 0.9855 - 7s/epoch - 17ms/step\n",
      "Epoch 68/200\n",
      "422/422 - 7s - loss: 0.0801 - accuracy: 0.9749 - val_loss: 0.0572 - val_accuracy: 0.9845 - 7s/epoch - 17ms/step\n",
      "Epoch 69/200\n",
      "422/422 - 7s - loss: 0.0802 - accuracy: 0.9748 - val_loss: 0.0598 - val_accuracy: 0.9837 - 7s/epoch - 17ms/step\n",
      "Epoch 70/200\n",
      "  >> Test@070 - loss: 0.0694 - acc: 0.9782\n",
      "422/422 - 7s - loss: 0.0794 - accuracy: 0.9744 - val_loss: 0.0579 - val_accuracy: 0.9842 - 7s/epoch - 17ms/step\n",
      "Epoch 71/200\n",
      "422/422 - 7s - loss: 0.0785 - accuracy: 0.9755 - val_loss: 0.0581 - val_accuracy: 0.9835 - 7s/epoch - 17ms/step\n",
      "Epoch 72/200\n",
      "422/422 - 7s - loss: 0.0794 - accuracy: 0.9741 - val_loss: 0.0565 - val_accuracy: 0.9837 - 7s/epoch - 17ms/step\n",
      "Epoch 73/200\n",
      "422/422 - 6s - loss: 0.0770 - accuracy: 0.9759 - val_loss: 0.0561 - val_accuracy: 0.9840 - 6s/epoch - 14ms/step\n",
      "Epoch 74/200\n",
      "422/422 - 6s - loss: 0.0743 - accuracy: 0.9766 - val_loss: 0.0557 - val_accuracy: 0.9842 - 6s/epoch - 14ms/step\n",
      "Epoch 75/200\n",
      "  >> Test@075 - loss: 0.0729 - acc: 0.9773\n",
      "422/422 - 7s - loss: 0.0757 - accuracy: 0.9764 - val_loss: 0.0590 - val_accuracy: 0.9830 - 7s/epoch - 16ms/step\n",
      "Epoch 76/200\n",
      "422/422 - 7s - loss: 0.0759 - accuracy: 0.9756 - val_loss: 0.0589 - val_accuracy: 0.9833 - 7s/epoch - 16ms/step\n",
      "Epoch 77/200\n",
      "422/422 - 7s - loss: 0.0740 - accuracy: 0.9762 - val_loss: 0.0563 - val_accuracy: 0.9842 - 7s/epoch - 16ms/step\n",
      "Epoch 78/200\n",
      "422/422 - 7s - loss: 0.0756 - accuracy: 0.9752 - val_loss: 0.0533 - val_accuracy: 0.9852 - 7s/epoch - 16ms/step\n",
      "Epoch 79/200\n",
      "422/422 - 7s - loss: 0.0764 - accuracy: 0.9758 - val_loss: 0.0521 - val_accuracy: 0.9853 - 7s/epoch - 16ms/step\n",
      "Epoch 80/200\n",
      "  >> Test@080 - loss: 0.0719 - acc: 0.9761\n",
      "422/422 - 7s - loss: 0.0730 - accuracy: 0.9769 - val_loss: 0.0538 - val_accuracy: 0.9865 - 7s/epoch - 17ms/step\n",
      "Epoch 81/200\n",
      "422/422 - 7s - loss: 0.0729 - accuracy: 0.9762 - val_loss: 0.0569 - val_accuracy: 0.9843 - 7s/epoch - 15ms/step\n",
      "Epoch 82/200\n",
      "422/422 - 7s - loss: 0.0715 - accuracy: 0.9772 - val_loss: 0.0539 - val_accuracy: 0.9852 - 7s/epoch - 16ms/step\n",
      "Epoch 83/200\n",
      "422/422 - 7s - loss: 0.0723 - accuracy: 0.9761 - val_loss: 0.0540 - val_accuracy: 0.9852 - 7s/epoch - 16ms/step\n",
      "Epoch 84/200\n",
      "422/422 - 7s - loss: 0.0724 - accuracy: 0.9767 - val_loss: 0.0502 - val_accuracy: 0.9863 - 7s/epoch - 16ms/step\n",
      "Epoch 85/200\n",
      "  >> Test@085 - loss: 0.0653 - acc: 0.9790\n",
      "422/422 - 7s - loss: 0.0717 - accuracy: 0.9773 - val_loss: 0.0510 - val_accuracy: 0.9845 - 7s/epoch - 17ms/step\n",
      "Epoch 86/200\n",
      "422/422 - 7s - loss: 0.0712 - accuracy: 0.9769 - val_loss: 0.0521 - val_accuracy: 0.9858 - 7s/epoch - 16ms/step\n",
      "Epoch 87/200\n",
      "422/422 - 7s - loss: 0.0701 - accuracy: 0.9782 - val_loss: 0.0534 - val_accuracy: 0.9847 - 7s/epoch - 16ms/step\n",
      "Epoch 88/200\n",
      "422/422 - 6s - loss: 0.0687 - accuracy: 0.9780 - val_loss: 0.0518 - val_accuracy: 0.9872 - 6s/epoch - 15ms/step\n",
      "Epoch 89/200\n",
      "422/422 - 7s - loss: 0.0685 - accuracy: 0.9782 - val_loss: 0.0527 - val_accuracy: 0.9862 - 7s/epoch - 16ms/step\n",
      "Epoch 90/200\n",
      "  >> Test@090 - loss: 0.0688 - acc: 0.9787\n",
      "422/422 - 7s - loss: 0.0693 - accuracy: 0.9778 - val_loss: 0.0520 - val_accuracy: 0.9852 - 7s/epoch - 17ms/step\n",
      "Epoch 91/200\n",
      "422/422 - 7s - loss: 0.0683 - accuracy: 0.9778 - val_loss: 0.0509 - val_accuracy: 0.9855 - 7s/epoch - 16ms/step\n",
      "Epoch 92/200\n",
      "422/422 - 7s - loss: 0.0674 - accuracy: 0.9785 - val_loss: 0.0512 - val_accuracy: 0.9860 - 7s/epoch - 16ms/step\n",
      "Epoch 93/200\n",
      "422/422 - 7s - loss: 0.0692 - accuracy: 0.9772 - val_loss: 0.0534 - val_accuracy: 0.9862 - 7s/epoch - 16ms/step\n",
      "Epoch 94/200\n",
      "422/422 - 7s - loss: 0.0689 - accuracy: 0.9782 - val_loss: 0.0543 - val_accuracy: 0.9848 - 7s/epoch - 16ms/step\n",
      "Epoch 95/200\n",
      "  >> Test@095 - loss: 0.0633 - acc: 0.9794\n",
      "422/422 - 7s - loss: 0.0669 - accuracy: 0.9788 - val_loss: 0.0529 - val_accuracy: 0.9853 - 7s/epoch - 17ms/step\n",
      "Epoch 96/200\n",
      "422/422 - 7s - loss: 0.0674 - accuracy: 0.9789 - val_loss: 0.0524 - val_accuracy: 0.9855 - 7s/epoch - 16ms/step\n",
      "Epoch 97/200\n",
      "422/422 - 7s - loss: 0.0668 - accuracy: 0.9795 - val_loss: 0.0511 - val_accuracy: 0.9852 - 7s/epoch - 16ms/step\n",
      "Epoch 98/200\n",
      "422/422 - 7s - loss: 0.0668 - accuracy: 0.9791 - val_loss: 0.0571 - val_accuracy: 0.9835 - 7s/epoch - 16ms/step\n",
      "Epoch 99/200\n",
      "422/422 - 7s - loss: 0.0650 - accuracy: 0.9792 - val_loss: 0.0528 - val_accuracy: 0.9837 - 7s/epoch - 16ms/step\n",
      "Epoch 100/200\n",
      "  >> Test@100 - loss: 0.0750 - acc: 0.9756\n",
      "422/422 - 7s - loss: 0.0671 - accuracy: 0.9786 - val_loss: 0.0528 - val_accuracy: 0.9853 - 7s/epoch - 17ms/step\n",
      "Epoch 101/200\n",
      "422/422 - 7s - loss: 0.0646 - accuracy: 0.9790 - val_loss: 0.0531 - val_accuracy: 0.9870 - 7s/epoch - 16ms/step\n",
      "Epoch 102/200\n",
      "422/422 - 7s - loss: 0.0647 - accuracy: 0.9794 - val_loss: 0.0544 - val_accuracy: 0.9852 - 7s/epoch - 16ms/step\n",
      "Epoch 103/200\n",
      "422/422 - 7s - loss: 0.0651 - accuracy: 0.9796 - val_loss: 0.0517 - val_accuracy: 0.9842 - 7s/epoch - 17ms/step\n",
      "Epoch 104/200\n",
      "422/422 - 7s - loss: 0.0649 - accuracy: 0.9790 - val_loss: 0.0528 - val_accuracy: 0.9848 - 7s/epoch - 16ms/step\n",
      "Epoch 105/200\n",
      "  >> Test@105 - loss: 0.0633 - acc: 0.9786\n",
      "422/422 - 7s - loss: 0.0648 - accuracy: 0.9795 - val_loss: 0.0509 - val_accuracy: 0.9852 - 7s/epoch - 17ms/step\n",
      "Epoch 106/200\n",
      "422/422 - 7s - loss: 0.0643 - accuracy: 0.9793 - val_loss: 0.0508 - val_accuracy: 0.9868 - 7s/epoch - 16ms/step\n",
      "Epoch 107/200\n",
      "422/422 - 7s - loss: 0.0656 - accuracy: 0.9785 - val_loss: 0.0489 - val_accuracy: 0.9865 - 7s/epoch - 16ms/step\n",
      "Epoch 108/200\n",
      "422/422 - 7s - loss: 0.0645 - accuracy: 0.9789 - val_loss: 0.0461 - val_accuracy: 0.9868 - 7s/epoch - 16ms/step\n",
      "Epoch 109/200\n",
      "422/422 - 7s - loss: 0.0624 - accuracy: 0.9797 - val_loss: 0.0495 - val_accuracy: 0.9863 - 7s/epoch - 16ms/step\n",
      "Epoch 110/200\n",
      "  >> Test@110 - loss: 0.0583 - acc: 0.9805\n",
      "422/422 - 7s - loss: 0.0638 - accuracy: 0.9798 - val_loss: 0.0494 - val_accuracy: 0.9855 - 7s/epoch - 17ms/step\n",
      "Epoch 111/200\n",
      "422/422 - 7s - loss: 0.0641 - accuracy: 0.9797 - val_loss: 0.0556 - val_accuracy: 0.9843 - 7s/epoch - 16ms/step\n",
      "Epoch 112/200\n",
      "422/422 - 7s - loss: 0.0627 - accuracy: 0.9805 - val_loss: 0.0471 - val_accuracy: 0.9852 - 7s/epoch - 16ms/step\n",
      "Epoch 113/200\n",
      "422/422 - 7s - loss: 0.0608 - accuracy: 0.9801 - val_loss: 0.0491 - val_accuracy: 0.9860 - 7s/epoch - 16ms/step\n",
      "Epoch 114/200\n",
      "422/422 - 7s - loss: 0.0616 - accuracy: 0.9803 - val_loss: 0.0513 - val_accuracy: 0.9862 - 7s/epoch - 17ms/step\n",
      "Epoch 115/200\n",
      "  >> Test@115 - loss: 0.0604 - acc: 0.9800\n",
      "422/422 - 7s - loss: 0.0616 - accuracy: 0.9800 - val_loss: 0.0524 - val_accuracy: 0.9857 - 7s/epoch - 17ms/step\n",
      "Epoch 116/200\n",
      "422/422 - 7s - loss: 0.0627 - accuracy: 0.9791 - val_loss: 0.0510 - val_accuracy: 0.9853 - 7s/epoch - 16ms/step\n",
      "Epoch 117/200\n",
      "422/422 - 7s - loss: 0.0598 - accuracy: 0.9806 - val_loss: 0.0521 - val_accuracy: 0.9847 - 7s/epoch - 16ms/step\n",
      "Epoch 118/200\n",
      "422/422 - 7s - loss: 0.0603 - accuracy: 0.9803 - val_loss: 0.0510 - val_accuracy: 0.9868 - 7s/epoch - 16ms/step\n",
      "Epoch 119/200\n",
      "422/422 - 7s - loss: 0.0603 - accuracy: 0.9795 - val_loss: 0.0492 - val_accuracy: 0.9857 - 7s/epoch - 16ms/step\n",
      "Epoch 120/200\n",
      "  >> Test@120 - loss: 0.0645 - acc: 0.9806\n",
      "422/422 - 7s - loss: 0.0619 - accuracy: 0.9808 - val_loss: 0.0517 - val_accuracy: 0.9847 - 7s/epoch - 17ms/step\n",
      "Epoch 121/200\n",
      "422/422 - 7s - loss: 0.0631 - accuracy: 0.9801 - val_loss: 0.0496 - val_accuracy: 0.9862 - 7s/epoch - 16ms/step\n",
      "Epoch 122/200\n",
      "422/422 - 7s - loss: 0.0596 - accuracy: 0.9811 - val_loss: 0.0469 - val_accuracy: 0.9875 - 7s/epoch - 16ms/step\n",
      "Epoch 123/200\n",
      "422/422 - 7s - loss: 0.0622 - accuracy: 0.9793 - val_loss: 0.0479 - val_accuracy: 0.9865 - 7s/epoch - 16ms/step\n",
      "Epoch 124/200\n",
      "422/422 - 7s - loss: 0.0608 - accuracy: 0.9801 - val_loss: 0.0487 - val_accuracy: 0.9862 - 7s/epoch - 16ms/step\n",
      "Epoch 125/200\n",
      "  >> Test@125 - loss: 0.0562 - acc: 0.9822\n",
      "422/422 - 7s - loss: 0.0604 - accuracy: 0.9802 - val_loss: 0.0485 - val_accuracy: 0.9850 - 7s/epoch - 17ms/step\n",
      "Epoch 126/200\n",
      "422/422 - 7s - loss: 0.0593 - accuracy: 0.9806 - val_loss: 0.0460 - val_accuracy: 0.9863 - 7s/epoch - 16ms/step\n",
      "Epoch 127/200\n",
      "422/422 - 7s - loss: 0.0591 - accuracy: 0.9815 - val_loss: 0.0471 - val_accuracy: 0.9865 - 7s/epoch - 16ms/step\n",
      "Epoch 128/200\n",
      "422/422 - 7s - loss: 0.0581 - accuracy: 0.9815 - val_loss: 0.0496 - val_accuracy: 0.9853 - 7s/epoch - 16ms/step\n",
      "Epoch 129/200\n",
      "422/422 - 7s - loss: 0.0581 - accuracy: 0.9812 - val_loss: 0.0468 - val_accuracy: 0.9872 - 7s/epoch - 16ms/step\n",
      "Epoch 130/200\n",
      "  >> Test@130 - loss: 0.0588 - acc: 0.9819\n",
      "422/422 - 7s - loss: 0.0586 - accuracy: 0.9806 - val_loss: 0.0448 - val_accuracy: 0.9872 - 7s/epoch - 17ms/step\n",
      "Epoch 131/200\n",
      "422/422 - 7s - loss: 0.0603 - accuracy: 0.9803 - val_loss: 0.0494 - val_accuracy: 0.9863 - 7s/epoch - 16ms/step\n",
      "Epoch 132/200\n",
      "422/422 - 6s - loss: 0.0595 - accuracy: 0.9814 - val_loss: 0.0482 - val_accuracy: 0.9867 - 6s/epoch - 14ms/step\n",
      "Epoch 133/200\n",
      "422/422 - 6s - loss: 0.0593 - accuracy: 0.9810 - val_loss: 0.0480 - val_accuracy: 0.9860 - 6s/epoch - 14ms/step\n",
      "Epoch 134/200\n",
      "422/422 - 6s - loss: 0.0601 - accuracy: 0.9804 - val_loss: 0.0489 - val_accuracy: 0.9857 - 6s/epoch - 14ms/step\n",
      "Epoch 135/200\n",
      "  >> Test@135 - loss: 0.0519 - acc: 0.9832\n",
      "422/422 - 6s - loss: 0.0577 - accuracy: 0.9817 - val_loss: 0.0477 - val_accuracy: 0.9865 - 6s/epoch - 14ms/step\n",
      "Epoch 136/200\n",
      "422/422 - 6s - loss: 0.0589 - accuracy: 0.9811 - val_loss: 0.0498 - val_accuracy: 0.9858 - 6s/epoch - 14ms/step\n",
      "Epoch 137/200\n",
      "422/422 - 6s - loss: 0.0582 - accuracy: 0.9813 - val_loss: 0.0477 - val_accuracy: 0.9858 - 6s/epoch - 13ms/step\n",
      "Epoch 138/200\n",
      "422/422 - 6s - loss: 0.0566 - accuracy: 0.9816 - val_loss: 0.0486 - val_accuracy: 0.9868 - 6s/epoch - 14ms/step\n",
      "Epoch 139/200\n",
      "422/422 - 6s - loss: 0.0576 - accuracy: 0.9809 - val_loss: 0.0478 - val_accuracy: 0.9857 - 6s/epoch - 14ms/step\n",
      "Epoch 140/200\n",
      "  >> Test@140 - loss: 0.0621 - acc: 0.9799\n",
      "422/422 - 6s - loss: 0.0544 - accuracy: 0.9830 - val_loss: 0.0486 - val_accuracy: 0.9850 - 6s/epoch - 14ms/step\n",
      "Epoch 141/200\n",
      "422/422 - 6s - loss: 0.0561 - accuracy: 0.9820 - val_loss: 0.0468 - val_accuracy: 0.9875 - 6s/epoch - 14ms/step\n",
      "Epoch 142/200\n",
      "422/422 - 6s - loss: 0.0563 - accuracy: 0.9825 - val_loss: 0.0494 - val_accuracy: 0.9865 - 6s/epoch - 13ms/step\n",
      "Epoch 143/200\n",
      "422/422 - 5s - loss: 0.0547 - accuracy: 0.9821 - val_loss: 0.0483 - val_accuracy: 0.9863 - 5s/epoch - 13ms/step\n",
      "Epoch 144/200\n",
      "422/422 - 6s - loss: 0.0588 - accuracy: 0.9811 - val_loss: 0.0507 - val_accuracy: 0.9852 - 6s/epoch - 14ms/step\n",
      "Epoch 145/200\n",
      "  >> Test@145 - loss: 0.0591 - acc: 0.9804\n",
      "422/422 - 6s - loss: 0.0563 - accuracy: 0.9816 - val_loss: 0.0479 - val_accuracy: 0.9858 - 6s/epoch - 14ms/step\n",
      "Epoch 146/200\n",
      "422/422 - 6s - loss: 0.0576 - accuracy: 0.9810 - val_loss: 0.0513 - val_accuracy: 0.9853 - 6s/epoch - 14ms/step\n",
      "Epoch 147/200\n",
      "422/422 - 6s - loss: 0.0556 - accuracy: 0.9821 - val_loss: 0.0463 - val_accuracy: 0.9865 - 6s/epoch - 14ms/step\n",
      "Epoch 148/200\n",
      "422/422 - 5s - loss: 0.0557 - accuracy: 0.9817 - val_loss: 0.0462 - val_accuracy: 0.9858 - 5s/epoch - 13ms/step\n",
      "Epoch 149/200\n",
      "422/422 - 6s - loss: 0.0550 - accuracy: 0.9821 - val_loss: 0.0487 - val_accuracy: 0.9867 - 6s/epoch - 13ms/step\n",
      "Epoch 150/200\n",
      "  >> Test@150 - loss: 0.0563 - acc: 0.9813\n",
      "422/422 - 6s - loss: 0.0571 - accuracy: 0.9814 - val_loss: 0.0462 - val_accuracy: 0.9865 - 6s/epoch - 14ms/step\n",
      "Epoch 151/200\n",
      "422/422 - 5s - loss: 0.0576 - accuracy: 0.9810 - val_loss: 0.0465 - val_accuracy: 0.9875 - 5s/epoch - 12ms/step\n",
      "Epoch 152/200\n",
      "422/422 - 5s - loss: 0.0574 - accuracy: 0.9816 - val_loss: 0.0491 - val_accuracy: 0.9863 - 5s/epoch - 12ms/step\n",
      "Epoch 153/200\n",
      "422/422 - 6s - loss: 0.0556 - accuracy: 0.9822 - val_loss: 0.0469 - val_accuracy: 0.9865 - 6s/epoch - 14ms/step\n",
      "Epoch 154/200\n",
      "422/422 - 6s - loss: 0.0561 - accuracy: 0.9824 - val_loss: 0.0458 - val_accuracy: 0.9870 - 6s/epoch - 14ms/step\n",
      "Epoch 155/200\n",
      "  >> Test@155 - loss: 0.0568 - acc: 0.9819\n",
      "422/422 - 6s - loss: 0.0548 - accuracy: 0.9817 - val_loss: 0.0443 - val_accuracy: 0.9882 - 6s/epoch - 15ms/step\n",
      "Epoch 156/200\n",
      "422/422 - 6s - loss: 0.0569 - accuracy: 0.9811 - val_loss: 0.0474 - val_accuracy: 0.9868 - 6s/epoch - 13ms/step\n",
      "Epoch 157/200\n",
      "422/422 - 5s - loss: 0.0555 - accuracy: 0.9825 - val_loss: 0.0493 - val_accuracy: 0.9870 - 5s/epoch - 12ms/step\n",
      "Epoch 158/200\n",
      "422/422 - 5s - loss: 0.0557 - accuracy: 0.9816 - val_loss: 0.0453 - val_accuracy: 0.9875 - 5s/epoch - 12ms/step\n",
      "Epoch 159/200\n",
      "422/422 - 5s - loss: 0.0558 - accuracy: 0.9822 - val_loss: 0.0457 - val_accuracy: 0.9877 - 5s/epoch - 13ms/step\n",
      "Epoch 160/200\n",
      "  >> Test@160 - loss: 0.0535 - acc: 0.9821\n",
      "422/422 - 6s - loss: 0.0567 - accuracy: 0.9815 - val_loss: 0.0456 - val_accuracy: 0.9863 - 6s/epoch - 14ms/step\n",
      "Epoch 161/200\n",
      "422/422 - 6s - loss: 0.0560 - accuracy: 0.9826 - val_loss: 0.0457 - val_accuracy: 0.9877 - 6s/epoch - 13ms/step\n",
      "Epoch 162/200\n",
      "422/422 - 6s - loss: 0.0540 - accuracy: 0.9825 - val_loss: 0.0451 - val_accuracy: 0.9878 - 6s/epoch - 13ms/step\n",
      "Epoch 163/200\n",
      "422/422 - 5s - loss: 0.0540 - accuracy: 0.9826 - val_loss: 0.0487 - val_accuracy: 0.9872 - 5s/epoch - 13ms/step\n",
      "Epoch 164/200\n",
      "422/422 - 5s - loss: 0.0541 - accuracy: 0.9824 - val_loss: 0.0499 - val_accuracy: 0.9860 - 5s/epoch - 13ms/step\n",
      "Epoch 165/200\n",
      "  >> Test@165 - loss: 0.0549 - acc: 0.9816\n",
      "422/422 - 6s - loss: 0.0532 - accuracy: 0.9833 - val_loss: 0.0464 - val_accuracy: 0.9877 - 6s/epoch - 13ms/step\n",
      "Epoch 166/200\n",
      "422/422 - 5s - loss: 0.0543 - accuracy: 0.9825 - val_loss: 0.0461 - val_accuracy: 0.9867 - 5s/epoch - 13ms/step\n",
      "Epoch 167/200\n",
      "422/422 - 5s - loss: 0.0550 - accuracy: 0.9822 - val_loss: 0.0441 - val_accuracy: 0.9870 - 5s/epoch - 13ms/step\n",
      "Epoch 168/200\n",
      "422/422 - 6s - loss: 0.0542 - accuracy: 0.9826 - val_loss: 0.0435 - val_accuracy: 0.9870 - 6s/epoch - 13ms/step\n",
      "Epoch 169/200\n",
      "422/422 - 6s - loss: 0.0532 - accuracy: 0.9826 - val_loss: 0.0444 - val_accuracy: 0.9877 - 6s/epoch - 13ms/step\n",
      "Epoch 170/200\n",
      "  >> Test@170 - loss: 0.0574 - acc: 0.9814\n",
      "422/422 - 6s - loss: 0.0544 - accuracy: 0.9824 - val_loss: 0.0445 - val_accuracy: 0.9865 - 6s/epoch - 14ms/step\n",
      "Epoch 171/200\n",
      "422/422 - 6s - loss: 0.0535 - accuracy: 0.9828 - val_loss: 0.0436 - val_accuracy: 0.9865 - 6s/epoch - 14ms/step\n",
      "Epoch 172/200\n",
      "422/422 - 6s - loss: 0.0532 - accuracy: 0.9824 - val_loss: 0.0449 - val_accuracy: 0.9863 - 6s/epoch - 13ms/step\n",
      "Epoch 173/200\n",
      "422/422 - 6s - loss: 0.0539 - accuracy: 0.9828 - val_loss: 0.0456 - val_accuracy: 0.9873 - 6s/epoch - 14ms/step\n",
      "Epoch 174/200\n",
      "422/422 - 6s - loss: 0.0555 - accuracy: 0.9819 - val_loss: 0.0438 - val_accuracy: 0.9875 - 6s/epoch - 13ms/step\n",
      "Epoch 175/200\n",
      "  >> Test@175 - loss: 0.0566 - acc: 0.9816\n",
      "422/422 - 6s - loss: 0.0555 - accuracy: 0.9818 - val_loss: 0.0434 - val_accuracy: 0.9875 - 6s/epoch - 13ms/step\n",
      "Epoch 176/200\n",
      "422/422 - 5s - loss: 0.0526 - accuracy: 0.9834 - val_loss: 0.0447 - val_accuracy: 0.9888 - 5s/epoch - 13ms/step\n",
      "Epoch 177/200\n",
      "422/422 - 6s - loss: 0.0550 - accuracy: 0.9818 - val_loss: 0.0439 - val_accuracy: 0.9885 - 6s/epoch - 13ms/step\n",
      "Epoch 178/200\n",
      "422/422 - 6s - loss: 0.0541 - accuracy: 0.9826 - val_loss: 0.0443 - val_accuracy: 0.9878 - 6s/epoch - 13ms/step\n",
      "Epoch 179/200\n",
      "422/422 - 6s - loss: 0.0511 - accuracy: 0.9836 - val_loss: 0.0449 - val_accuracy: 0.9880 - 6s/epoch - 14ms/step\n",
      "Epoch 180/200\n",
      "  >> Test@180 - loss: 0.0546 - acc: 0.9829\n",
      "422/422 - 6s - loss: 0.0534 - accuracy: 0.9828 - val_loss: 0.0452 - val_accuracy: 0.9882 - 6s/epoch - 14ms/step\n",
      "Epoch 181/200\n",
      "422/422 - 6s - loss: 0.0539 - accuracy: 0.9825 - val_loss: 0.0457 - val_accuracy: 0.9873 - 6s/epoch - 14ms/step\n",
      "Epoch 182/200\n",
      "422/422 - 6s - loss: 0.0520 - accuracy: 0.9837 - val_loss: 0.0449 - val_accuracy: 0.9877 - 6s/epoch - 14ms/step\n",
      "Epoch 183/200\n",
      "422/422 - 5s - loss: 0.0519 - accuracy: 0.9832 - val_loss: 0.0434 - val_accuracy: 0.9885 - 5s/epoch - 13ms/step\n",
      "Epoch 184/200\n",
      "422/422 - 6s - loss: 0.0515 - accuracy: 0.9826 - val_loss: 0.0419 - val_accuracy: 0.9878 - 6s/epoch - 13ms/step\n",
      "Epoch 185/200\n",
      "  >> Test@185 - loss: 0.0544 - acc: 0.9825\n",
      "422/422 - 5s - loss: 0.0509 - accuracy: 0.9839 - val_loss: 0.0429 - val_accuracy: 0.9872 - 5s/epoch - 13ms/step\n",
      "Epoch 186/200\n",
      "422/422 - 5s - loss: 0.0519 - accuracy: 0.9829 - val_loss: 0.0437 - val_accuracy: 0.9882 - 5s/epoch - 13ms/step\n",
      "Epoch 187/200\n",
      "422/422 - 5s - loss: 0.0504 - accuracy: 0.9836 - val_loss: 0.0430 - val_accuracy: 0.9873 - 5s/epoch - 12ms/step\n",
      "Epoch 188/200\n",
      "422/422 - 5s - loss: 0.0495 - accuracy: 0.9833 - val_loss: 0.0447 - val_accuracy: 0.9883 - 5s/epoch - 12ms/step\n",
      "Epoch 189/200\n",
      "422/422 - 5s - loss: 0.0512 - accuracy: 0.9834 - val_loss: 0.0426 - val_accuracy: 0.9873 - 5s/epoch - 12ms/step\n",
      "Epoch 190/200\n",
      "  >> Test@190 - loss: 0.0567 - acc: 0.9806\n",
      "422/422 - 5s - loss: 0.0528 - accuracy: 0.9833 - val_loss: 0.0462 - val_accuracy: 0.9873 - 5s/epoch - 13ms/step\n",
      "Epoch 191/200\n",
      "422/422 - 5s - loss: 0.0529 - accuracy: 0.9832 - val_loss: 0.0449 - val_accuracy: 0.9873 - 5s/epoch - 12ms/step\n",
      "Epoch 192/200\n",
      "422/422 - 5s - loss: 0.0522 - accuracy: 0.9830 - val_loss: 0.0447 - val_accuracy: 0.9872 - 5s/epoch - 13ms/step\n",
      "Epoch 193/200\n",
      "422/422 - 5s - loss: 0.0535 - accuracy: 0.9824 - val_loss: 0.0420 - val_accuracy: 0.9863 - 5s/epoch - 12ms/step\n",
      "Epoch 194/200\n",
      "422/422 - 5s - loss: 0.0515 - accuracy: 0.9837 - val_loss: 0.0406 - val_accuracy: 0.9875 - 5s/epoch - 12ms/step\n",
      "Epoch 195/200\n",
      "  >> Test@195 - loss: 0.0519 - acc: 0.9817\n",
      "422/422 - 6s - loss: 0.0532 - accuracy: 0.9827 - val_loss: 0.0418 - val_accuracy: 0.9875 - 6s/epoch - 13ms/step\n",
      "Epoch 196/200\n",
      "422/422 - 5s - loss: 0.0503 - accuracy: 0.9844 - val_loss: 0.0411 - val_accuracy: 0.9880 - 5s/epoch - 12ms/step\n",
      "Epoch 197/200\n",
      "422/422 - 5s - loss: 0.0535 - accuracy: 0.9830 - val_loss: 0.0446 - val_accuracy: 0.9872 - 5s/epoch - 13ms/step\n",
      "Epoch 198/200\n",
      "422/422 - 5s - loss: 0.0525 - accuracy: 0.9830 - val_loss: 0.0417 - val_accuracy: 0.9882 - 5s/epoch - 12ms/step\n",
      "Epoch 199/200\n",
      "422/422 - 5s - loss: 0.0528 - accuracy: 0.9831 - val_loss: 0.0434 - val_accuracy: 0.9877 - 5s/epoch - 12ms/step\n",
      "Epoch 200/200\n",
      "  >> Test@200 - loss: 0.0534 - acc: 0.9830\n",
      "422/422 - 6s - loss: 0.0516 - accuracy: 0.9835 - val_loss: 0.0429 - val_accuracy: 0.9875 - 6s/epoch - 13ms/step\n",
      "Done: SET (IMP, Î±=0.75) | Final TestAcc=0.9830 | nW=128876\n",
      "\n",
      " Final Table\n",
      "Dense MLP                        | nW=2797010    | TestAcc=0.9851 | BestVal=0.9875 | Time=210.4s\n",
      "FixProb                          | nW=128725     | TestAcc=0.9808 | BestVal=0.9853 | Time=1136.1s\n",
      "SET (RAND)                       | nW=128564     | TestAcc=0.9850 | BestVal=0.9875 | Time=1075.3s\n",
      "SET (IMP, Î±=0.75)                | nW=128876     | TestAcc=0.9830 | BestVal=0.9888 | Time=1299.9s\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    compare_all_models(\n",
    "        eps=20,\n",
    "        zeta=0.3,\n",
    "        alpha=0.75,\n",
    "        epochs=200,\n",
    "        seed=42,\n",
    "        weight_decay=2e-4,\n",
    "        dropout=0.3\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a3692c-d377-4676-9d9b-f5cd1e7cf01e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
